{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "small-genetics",
   "metadata": {},
   "source": [
    "# MinPrj_Lyrics\n",
    "\n",
    "date : 5 october 2021\n",
    "author : bae hueng myoung\n",
    "prj goals : \n",
    "    1)\n",
    "    2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "registered-badge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Can we forget about the things I said when I was drunk...', \"I didn't mean to call you that\", \"I can't remember what was said\"]\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "corporate-paragraph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/aiffel/aiffel/lyricist/data/blink-182.txt', '/aiffel/aiffel/lyricist/data/al-green.txt', '/aiffel/aiffel/lyricist/data/johnny-cash.txt', '/aiffel/aiffel/lyricist/data/r-kelly.txt', '/aiffel/aiffel/lyricist/data/nirvana.txt', '/aiffel/aiffel/lyricist/data/dickinson.txt', '/aiffel/aiffel/lyricist/data/michael-jackson.txt', '/aiffel/aiffel/lyricist/data/paul-simon.txt', '/aiffel/aiffel/lyricist/data/prince.txt', '/aiffel/aiffel/lyricist/data/nicki-minaj.txt', '/aiffel/aiffel/lyricist/data/adele.txt', '/aiffel/aiffel/lyricist/data/Lil_Wayne.txt', '/aiffel/aiffel/lyricist/data/lorde.txt', '/aiffel/aiffel/lyricist/data/dj-khaled.txt', '/aiffel/aiffel/lyricist/data/bob-marley.txt', '/aiffel/aiffel/lyricist/data/alicia-keys.txt', '/aiffel/aiffel/lyricist/data/janisjoplin.txt', '/aiffel/aiffel/lyricist/data/lady-gaga.txt', '/aiffel/aiffel/lyricist/data/dolly-parton.txt', '/aiffel/aiffel/lyricist/data/britney-spears.txt', '/aiffel/aiffel/lyricist/data/lin-manuel-miranda.txt', '/aiffel/aiffel/lyricist/data/lil-wayne.txt', '/aiffel/aiffel/lyricist/data/notorious_big.txt', '/aiffel/aiffel/lyricist/data/eminem.txt', '/aiffel/aiffel/lyricist/data/bjork.txt', '/aiffel/aiffel/lyricist/data/notorious-big.txt', '/aiffel/aiffel/lyricist/data/missy-elliott.txt', '/aiffel/aiffel/lyricist/data/kanye-west.txt', '/aiffel/aiffel/lyricist/data/bruno-mars.txt', '/aiffel/aiffel/lyricist/data/nickelback.txt', '/aiffel/aiffel/lyricist/data/ludacris.txt', '/aiffel/aiffel/lyricist/data/joni-mitchell.txt', '/aiffel/aiffel/lyricist/data/jimi-hendrix.txt', '/aiffel/aiffel/lyricist/data/patti-smith.txt', '/aiffel/aiffel/lyricist/data/Kanye_West.txt', '/aiffel/aiffel/lyricist/data/kanye.txt', '/aiffel/aiffel/lyricist/data/cake.txt', '/aiffel/aiffel/lyricist/data/nursery_rhymes.txt', '/aiffel/aiffel/lyricist/data/bieber.txt', '/aiffel/aiffel/lyricist/data/disney.txt', '/aiffel/aiffel/lyricist/data/drake.txt', '/aiffel/aiffel/lyricist/data/radiohead.txt', '/aiffel/aiffel/lyricist/data/amy-winehouse.txt', '/aiffel/aiffel/lyricist/data/rihanna.txt', '/aiffel/aiffel/lyricist/data/leonard-cohen.txt', '/aiffel/aiffel/lyricist/data/dr-seuss.txt', '/aiffel/aiffel/lyricist/data/bob-dylan.txt', '/aiffel/aiffel/lyricist/data/beatles.txt', '/aiffel/aiffel/lyricist/data/bruce-springsteen.txt']\n"
     ]
    }
   ],
   "source": [
    "print(txt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-closing",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "    1) 불필요한 문자 제거\n",
    "    2) length < 13 : 토큰화 갯수가 15개 이하를 만족하기 위해선 각 문장의 공백은 13개 이하가 되어야 함\n",
    "    3) generating token\n",
    "    4) word frequency caculation / 입려된 문장들의 단어들이 얼마나의 빈도를 갖는지 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prompt-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 불필요한 문자 제거\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "structural-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187088\n",
      "174144\n",
      "['Can we forget about the things I said when I was drunk...', \"I didn't mean to call you that\", \"I can't remember what was said\", 'Or what you threw at me Please tell me', 'Please tell me why', 'My car is in the front yard', 'And I am sleeping with my cloths on', 'I came in throught the window... Last night', 'And your... Gone', \"Gone It's no suprise to me I am my own worst enemy\"]\n"
     ]
    }
   ],
   "source": [
    "# 2) length < 13 : 토큰화 갯수가 15개 이하를 만족하기 위해선 각 문장의 공백은 13개 이하가 되어야 함\n",
    "def blank_num_calc(sentence):\n",
    "    cnt = 0\n",
    "    for ele in sentence:\n",
    "        if ele == ' ':\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "    \n",
    "    \n",
    "lenmodcorp = []\n",
    "for sen in raw_corpus:\n",
    "    if blank_num_calc(sen) < 13:\n",
    "        lenmodcorp.append(sen)\n",
    "\n",
    "print(len(raw_corpus))\n",
    "print(len(lenmodcorp))\n",
    "print(lenmodcorp[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accompanied-fighter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> can we forget about the things i said when i was drunk . . . <end>',\n",
       " '<start> i didn t mean to call you that <end>',\n",
       " '<start> i can t remember what was said <end>',\n",
       " '<start> or what you threw at me please tell me <end>',\n",
       " '<start> please tell me why <end>',\n",
       " '<start> my car is in the front yard <end>',\n",
       " '<start> and i am sleeping with my cloths on <end>',\n",
       " '<start> i came in throught the window . . . last night <end>',\n",
       " '<start> and your . . . gone <end>',\n",
       " '<start> gone it s no suprise to me i am my own worst enemy <end>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence_ele in lenmodcorp:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence_ele) == 0: continue\n",
    "    if sentence_ele[-1] == \":\": continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence_ele)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "oriented-cookbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155793\n"
     ]
    }
   ],
   "source": [
    "# if token num > 15, remove sentence!\n",
    "corpus_new = []\n",
    "for sen in corpus:\n",
    "    if blank_num_calc(sen) < 15:\n",
    "        corpus_new.append(sen)\n",
    "        \n",
    "print(len(corpus_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "emerging-fossil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155793, 15)\n",
      "<keras_preprocessing.text.Tokenizer object at 0x7f1d59bb4650>\n"
     ]
    }
   ],
   "source": [
    "# 3) generating token\n",
    "# making num_words = 12000\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus_new)\n",
    "tensor = tensor[:,:15]\n",
    "print(tensor.shape)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "social-ordering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : that\n",
      "16 : t\n",
      "17 : s\n",
      "18 : on\n",
      "19 : your\n",
      "20 : of\n"
     ]
    }
   ],
   "source": [
    "word_list = tokenizer.index_word\n",
    "\n",
    "for idx in word_list:\n",
    "    print(idx, \":\", word_list[idx])\n",
    "    if idx >= 20: break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "impressed-wrist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> 0\n",
      "<start> 162813\n",
      "<end> 162813\n",
      ", 53789\n",
      "i 50661\n",
      "the 41496\n",
      "you 38345\n",
      "and 24463\n",
      "a 22205\n",
      "to 21802\n",
      "it 18449\n",
      "me 17212\n",
      "my 16256\n",
      "in 14029\n",
      "t 13264\n",
      "s 12974\n",
      "that 12554\n",
      "on 10488\n",
      ". 9697\n",
      "your 9576\n",
      "of 9511\n",
      "we 9146\n",
      "m 9102\n",
      "like 8301\n",
      "all 8101\n",
      "is 7699\n",
      "be 7205\n",
      "for 6928\n",
      "up 6837\n",
      "with 6631\n",
      "so 6560\n",
      "can 6233\n",
      "but 6219\n",
      "know 6216\n",
      "just 6211\n",
      "don 6163\n",
      "love 6017\n",
      "no 5874\n",
      "what 5723\n",
      "got 5714\n",
      "oh 5568\n",
      "they 5530\n",
      "this 5484\n",
      "? 5455\n",
      "get 5346\n",
      "she 5138\n",
      "when 5065\n",
      "do 4888\n",
      "yeah 4710\n",
      "now 4374\n",
      "baby 4314\n",
      "re 4294\n",
      "if 4257\n",
      "go 4134\n",
      "he 4115\n",
      "out 4036\n",
      "was 3962\n",
      "! 3845\n",
      "down 3755\n",
      "one 3635\n",
      "ll 3509\n",
      "there 3271\n",
      "see 3185\n",
      "let 3177\n",
      "want 3129\n",
      "cause 3099\n",
      "come 3093\n",
      "not 3066\n",
      "her 3042\n",
      "say 2990\n",
      "at 2962\n",
      "make 2943\n",
      "from 2798\n",
      "time 2793\n",
      "have 2742\n",
      "are 2720\n",
      "back 2711\n",
      "how 2609\n",
      "never 2555\n",
      "girl 2501\n",
      "im 2490\n",
      "take 2454\n",
      "as 2439\n",
      "man 2331\n",
      "will 2314\n",
      "right 2286\n",
      "way 2256\n",
      "wanna 2223\n",
      "u 2179\n",
      "ain 2157\n",
      "ve 2108\n",
      "here 2071\n",
      "need 2070\n",
      "then 2034\n",
      "who 2004\n",
      "tell 1998\n",
      "some 1956\n",
      "more 1925\n",
      "gonna 1925\n",
      "where 1877\n",
      "his 1821\n",
      "too 1819\n",
      "been 1789\n",
      "could 1771\n",
      "feel 1770\n",
      "them 1759\n",
      "life 1739\n",
      "or 1714\n",
      "good 1689\n",
      "shit 1664\n",
      "said 1638\n",
      "give 1637\n",
      "about 1626\n",
      "why 1626\n",
      "night 1604\n",
      "had 1604\n",
      "little 1586\n",
      "off 1585\n",
      "by 1565\n",
      "day 1555\n",
      "ya 1501\n",
      "only 1496\n",
      "keep 1465\n",
      "still 1458\n",
      "em 1451\n",
      "every 1447\n",
      "us 1444\n",
      "think 1440\n",
      "bitch 1426\n",
      "through 1413\n",
      "fuck 1411\n",
      "d 1402\n",
      "look 1393\n",
      "these 1356\n",
      "would 1342\n",
      "around 1342\n",
      "away 1337\n",
      "heart 1311\n",
      "hey 1309\n",
      "its 1295\n",
      "money 1291\n",
      "over 1274\n",
      "him 1265\n",
      "world 1262\n",
      "well 1256\n",
      "dont 1255\n",
      "gotta 1223\n",
      "la 1222\n",
      "better 1205\n",
      "put 1200\n",
      "an 1200\n",
      "our 1178\n",
      "call 1170\n",
      "new 1155\n",
      "am 1141\n",
      "bad 1120\n",
      "boy 1116\n",
      "even 1111\n",
      "ever 1073\n",
      "stop 1069\n",
      "aint 1060\n",
      "really 1057\n",
      "home 1054\n",
      "nigga 1047\n",
      "won 1045\n",
      "long 1034\n",
      "than 1022\n",
      "always 1000\n",
      "did 998\n",
      "big 989\n",
      "nothing 985\n",
      "niggas 981\n",
      "mind 971\n",
      "people 963\n",
      "things 958\n",
      "before 958\n",
      "everything 955\n",
      "were 949\n",
      "again 947\n",
      "everybody 937\n",
      "head 934\n",
      "ooh 933\n",
      "much 912\n",
      "hold 907\n",
      "thing 902\n",
      "show 894\n",
      "hear 884\n",
      "da 868\n",
      "god 864\n",
      "gone 857\n",
      "leave 855\n",
      "name 855\n",
      "yo 850\n",
      "ass 843\n",
      "should 839\n",
      "something 835\n",
      "eyes 829\n",
      "hard 826\n",
      "turn 825\n",
      "live 819\n",
      "going 808\n",
      "another 808\n",
      "real 800\n",
      "hit 800\n",
      "find 786\n",
      "face 781\n",
      "talk 780\n",
      "try 775\n",
      "old 759\n",
      "high 756\n",
      "play 755\n",
      "body 754\n",
      "uh 739\n",
      "light 731\n",
      "alone 725\n",
      "stay 718\n",
      "believe 717\n",
      "please 710\n",
      "two 709\n",
      "mine 702\n",
      "into 694\n",
      "made 693\n",
      "nixga 693\n",
      "dance 692\n",
      "na 691\n",
      "work 684\n",
      "told 680\n",
      "last 679\n",
      "tonight 678\n",
      "best 671\n",
      "hot 665\n",
      "hand 664\n",
      "their 660\n",
      "bout 657\n",
      "thats 655\n",
      "while 649\n",
      "other 649\n",
      "yes 634\n",
      "nobody 632\n",
      "black 619\n",
      "gon 617\n",
      "left 614\n",
      "ah 613\n",
      "came 611\n",
      "round 606\n",
      "die 604\n",
      "cry 601\n",
      "mean 598\n",
      "bitches 596\n",
      "somebody 595\n",
      "fly 592\n",
      "chorus 590\n",
      "place 588\n",
      "wait 587\n",
      "run 586\n",
      "first 582\n",
      "without 582\n",
      "stand 582\n",
      "enough 578\n",
      "whole 578\n",
      "done 576\n",
      "free 569\n",
      "girls 567\n",
      "sky 563\n",
      "friends 562\n",
      "hands 557\n",
      "young 556\n",
      "alright 552\n",
      "same 551\n",
      "cant 550\n",
      "wrong 547\n",
      "thought 542\n",
      "has 540\n",
      "damn 539\n",
      "crazy 537\n",
      "must 532\n",
      "together 528\n",
      "wish 523\n",
      "inside 523\n",
      "whoa 520\n",
      "maybe 517\n",
      "game 515\n",
      "coming 514\n",
      "care 513\n",
      "own 512\n",
      "fire 509\n",
      "goin 505\n",
      "went 502\n",
      "til 498\n",
      "ready 496\n",
      "rock 493\n",
      "looking 492\n",
      "fall 483\n",
      "party 483\n",
      "y 483\n",
      "break 481\n",
      "sun 480\n",
      "used 480\n",
      "move 477\n",
      "touch 473\n",
      "myself 468\n",
      "many 462\n",
      "friend 462\n",
      "trying 460\n",
      "ride 459\n",
      "might 457\n",
      "heard 456\n",
      "ask 455\n",
      "soul 455\n",
      "remember 454\n",
      "bring 454\n",
      "knows 453\n",
      "because 451\n",
      "seen 451\n",
      "help 449\n",
      "end 448\n",
      "forever 448\n",
      "watch 447\n",
      "side 447\n",
      "sing 447\n",
      "guess 446\n",
      "lost 444\n",
      "door 444\n",
      "till 442\n",
      "fuckin 440\n",
      "someone 439\n",
      "white 437\n",
      "pussy 435\n",
      "though 434\n",
      "lie 433\n",
      "next 427\n",
      "wit 426\n",
      "house 425\n",
      "music 425\n",
      "mama 424\n",
      "feeling 423\n",
      "walk 423\n",
      "change 420\n",
      "beat 417\n",
      "true 415\n",
      "cold 412\n",
      "start 409\n",
      "goes 408\n",
      "dream 407\n",
      "took 407\n",
      "matter 407\n",
      "ma 405\n",
      "hope 402\n",
      "morning 401\n",
      "sweet 398\n",
      "o 398\n",
      "days 397\n",
      "after 395\n",
      "lord 395\n",
      "far 394\n",
      "may 394\n",
      "getting 393\n",
      "huh 393\n",
      "floor 391\n",
      "didn 389\n",
      "kiss 388\n",
      "waiting 386\n",
      "nixgas 386\n",
      "song 385\n",
      "rain 385\n",
      "yall 383\n",
      "until 381\n",
      "blue 379\n",
      "room 378\n",
      "top 378\n",
      "miss 376\n",
      "honey 373\n",
      "woman 373\n",
      "sometimes 372\n",
      "town 371\n",
      "any 371\n",
      "knew 371\n",
      "since 370\n",
      "biggie 370\n",
      "smile 369\n",
      "happy 368\n",
      "blow 367\n",
      "daddy 367\n",
      "bed 364\n",
      "dreams 363\n",
      "shot 361\n",
      "lights 360\n",
      "hate 359\n",
      "beautiful 359\n",
      "babe 359\n",
      "gettin 358\n",
      "boys 357\n",
      "drop 356\n",
      "deep 354\n",
      "pretty 353\n",
      "comes 352\n",
      "hair 351\n",
      "shake 350\n",
      "each 350\n",
      "eh 349\n",
      "dead 347\n",
      "those 347\n",
      "under 346\n",
      "lose 346\n",
      "once 346\n",
      "dick 345\n",
      "lay 345\n",
      "three 344\n",
      "times 343\n",
      "heaven 342\n",
      "throw 342\n",
      "roll 341\n",
      "hell 338\n",
      "close 338\n",
      "easy 336\n",
      "hoes 335\n",
      "talking 334\n",
      "alive 333\n",
      "pain 333\n",
      "ill 333\n",
      "city 332\n",
      "late 331\n",
      "hurt 331\n",
      "fight 330\n",
      "living 329\n",
      "low 329\n",
      "full 328\n",
      "fucking 328\n",
      "gimme 328\n",
      "understand 327\n",
      "cut 324\n",
      "road 322\n",
      "forget 320\n",
      "ha 320\n",
      "boom 320\n",
      "straight 319\n",
      "lot 316\n",
      "pull 316\n",
      "club 315\n",
      "words 314\n",
      "car 312\n",
      "years 312\n",
      "slow 312\n",
      "yourself 312\n",
      "check 311\n",
      "phone 310\n",
      "line 309\n",
      "gave 308\n",
      "sure 308\n",
      "red 308\n",
      "pay 307\n",
      "nasty 307\n",
      "lonely 306\n",
      "dark 305\n",
      "sleep 305\n",
      "eat 304\n",
      "open 302\n",
      "feels 301\n",
      "arms 301\n",
      "behind 300\n",
      "streets 299\n",
      "doing 297\n",
      "feet 296\n",
      "buy 296\n",
      "lookin 296\n",
      "mother 294\n",
      "x 294\n",
      "anything 292\n",
      "talkin 291\n",
      "wild 290\n",
      "word 288\n",
      "c 288\n",
      "n 288\n",
      "drink 287\n",
      "makes 287\n",
      "sex 287\n",
      "yet 286\n",
      "water 286\n",
      "air 285\n",
      "nah 284\n",
      "says 282\n",
      "meet 280\n",
      "gold 280\n",
      "saw 280\n",
      "million 279\n",
      "else 279\n",
      "listen 278\n",
      "today 276\n",
      "such 275\n",
      "fast 275\n",
      "nothin 275\n",
      "found 274\n",
      "wonder 273\n",
      "lets 272\n",
      "saying 271\n",
      "king 268\n",
      "set 267\n",
      "lady 267\n",
      "tryin 266\n",
      "broken 265\n",
      "act 265\n",
      "ho 265\n",
      "very 264\n",
      "pop 264\n",
      "hood 262\n",
      "kill 261\n",
      "verse 261\n",
      "street 259\n",
      "loving 259\n",
      "cool 258\n",
      "lil 258\n",
      "thinking 257\n",
      "feelin 256\n",
      "tryna 256\n",
      "smoke 255\n",
      "mad 255\n",
      "star 254\n",
      "tried 253\n",
      "swear 252\n",
      "mma 251\n",
      "kind 250\n",
      "wanted 250\n",
      "comin 250\n",
      "upon 249\n",
      "both 249\n",
      "doin 248\n",
      "g 248\n",
      "met 247\n",
      "tears 247\n",
      "mouth 246\n",
      "tight 245\n",
      "blood 245\n",
      "dog 245\n",
      "past 244\n",
      "wake 244\n",
      "being 244\n",
      "pray 244\n",
      "hello 243\n",
      "moon 243\n",
      "between 242\n",
      "gun 242\n",
      "use 241\n",
      "sorry 241\n",
      "rap 241\n",
      "summer 239\n",
      "kids 239\n",
      "soon 239\n",
      "flow 238\n",
      "sea 237\n",
      "chance 236\n",
      "fine 236\n",
      "drive 236\n",
      "shine 235\n",
      "started 234\n",
      "child 234\n",
      "truth 233\n",
      "fun 233\n",
      "clothes 232\n",
      "broke 232\n",
      "pick 232\n",
      "motherfucker 232\n",
      "women 232\n",
      "wind 232\n",
      "youre 232\n",
      "strong 231\n",
      "sit 231\n",
      "school 231\n",
      "fool 231\n",
      "making 230\n",
      "b 230\n",
      "eye 229\n",
      "lover 229\n",
      "push 229\n",
      "half 228\n",
      "save 227\n",
      "ground 227\n",
      "motherfuckers 227\n",
      "pass 227\n",
      "control 227\n",
      "wall 226\n",
      "reason 226\n",
      "mr 226\n",
      "brother 226\n",
      "year 225\n",
      "seems 225\n",
      "nixgaz 225\n",
      "whatever 224\n",
      "death 223\n",
      "chick 223\n",
      "felt 222\n",
      "bag 222\n",
      "children 221\n",
      "power 221\n",
      "bright 219\n",
      "shall 218\n",
      "sound 218\n",
      "somethin 218\n",
      "front 216\n",
      "ay 216\n",
      "number 215\n",
      "couldn 214\n",
      "standing 213\n",
      "read 213\n",
      "four 213\n",
      "rest 212\n",
      "along 211\n",
      "taking 210\n",
      "step 209\n",
      "e 209\n",
      "part 207\n",
      "perfect 207\n",
      "burn 206\n",
      "tomorrow 206\n",
      "wants 206\n",
      "does 206\n",
      "send 206\n",
      "funk 206\n",
      "green 204\n",
      "son 204\n",
      "sick 203\n",
      "different 203\n",
      "darling 202\n",
      "bought 201\n",
      "l 201\n",
      "loved 201\n",
      "win 200\n",
      "catch 200\n",
      "already 199\n",
      "window 198\n",
      "okay 198\n",
      "gets 197\n",
      "caught 197\n",
      "thank 197\n",
      "rich 197\n",
      "jesus 196\n",
      "father 196\n",
      "r 196\n",
      "stars 195\n",
      "most 195\n",
      "sayin 195\n",
      "loves 194\n",
      "ice 194\n",
      "wont 194\n",
      "jack 193\n",
      "tired 193\n",
      "men 193\n",
      "block 193\n",
      "running 192\n",
      "bet 192\n",
      "apart 191\n",
      "doesn 191\n",
      "hundred 191\n",
      "earth 190\n",
      "trust 190\n",
      "grab 190\n",
      "five 190\n",
      "cash 190\n",
      "called 189\n",
      "stupid 189\n",
      "rise 189\n",
      "land 189\n",
      "keys 189\n",
      "whats 188\n",
      "train 186\n",
      "family 186\n",
      "lies 185\n",
      "seem 185\n",
      "afraid 185\n",
      "fell 184\n",
      "shut 184\n",
      "bit 183\n",
      "wouldn 182\n",
      "fear 181\n",
      "lips 181\n",
      "moment 181\n",
      "promise 181\n",
      "de 181\n",
      "story 180\n",
      "cuz 178\n",
      "ladies 178\n",
      "nice 177\n",
      "follow 177\n",
      "ring 177\n",
      "yea 177\n",
      "born 177\n",
      "war 176\n",
      "voice 175\n",
      "speak 175\n",
      "woo 175\n",
      "bang 175\n",
      "york 174\n",
      "which 174\n",
      "imma 174\n",
      "missy 174\n",
      "bust 173\n",
      "scream 172\n",
      "above 172\n",
      "bone 172\n",
      "dirty 172\n",
      "style 172\n",
      "yours 171\n",
      "river 170\n",
      "sexy 170\n",
      "brain 170\n",
      "holy 170\n",
      "wear 169\n",
      "everyone 169\n",
      "calling 169\n",
      "thousand 169\n",
      "space 168\n",
      "minute 167\n",
      "blame 167\n",
      "worry 167\n",
      "ok 167\n",
      "freak 167\n",
      "ball 167\n",
      "mon 166\n",
      "against 165\n",
      "taste 165\n",
      "wasn 165\n",
      "across 165\n",
      "takes 165\n",
      "spit 165\n",
      "couple 165\n",
      "diamonds 165\n",
      "laugh 164\n",
      "twenty 163\n",
      "six 163\n",
      "walking 163\n",
      "welcome 163\n",
      "bum 163\n",
      "joy 161\n",
      "spend 161\n",
      "drunk 160\n",
      "prayer 160\n",
      "sign 158\n",
      "higher 158\n",
      "shoot 158\n",
      "paper 158\n",
      "livin 157\n",
      "diamond 157\n",
      "hide 156\n",
      "brown 156\n",
      "scared 155\n",
      "future 155\n",
      "died 155\n",
      "cars 155\n",
      "looks 155\n",
      "ten 154\n",
      "cannot 154\n",
      "skin 154\n",
      "lovin 154\n",
      "everywhere 154\n",
      "p 154\n",
      "reach 153\n",
      "almost 153\n",
      "dj 153\n",
      "lift 152\n",
      "learn 152\n",
      "great 152\n",
      "treat 152\n",
      "ones 152\n",
      "crack 152\n",
      "shoes 151\n",
      "looked 151\n",
      "track 151\n",
      "kick 150\n",
      "tree 150\n",
      "meant 150\n",
      "turned 150\n",
      "weed 150\n",
      "yah 150\n",
      "naked 149\n",
      "write 149\n",
      "runnin 149\n",
      "momma 149\n",
      "fresh 149\n",
      "business 149\n",
      "breath 148\n",
      "picture 148\n",
      "outta 148\n",
      "makin 148\n",
      "second 147\n",
      "breathe 146\n",
      "sad 146\n",
      "michael 146\n",
      "probably 145\n",
      "kinda 145\n",
      "goodbye 145\n",
      "somewhere 145\n",
      "queen 144\n",
      "outside 143\n",
      "giving 143\n",
      "key 143\n",
      "stick 142\n",
      "least 142\n",
      "ye 142\n",
      "supposed 141\n",
      "singing 141\n",
      "favorite 141\n",
      "keeps 141\n",
      "bridge 141\n",
      "brand 141\n",
      "human 141\n",
      "mirror 140\n",
      "ran 140\n",
      "hoe 140\n",
      "nights 139\n",
      "fucked 139\n",
      "guy 139\n",
      "few 139\n",
      "falling 139\n",
      "fill 139\n",
      "thinkin 139\n",
      "jump 139\n",
      "cross 139\n",
      "homie 139\n",
      "dancing 139\n",
      "dear 138\n",
      "wings 138\n",
      "piece 138\n",
      "playing 137\n",
      "anybody 137\n",
      "stone 136\n",
      "callin 136\n",
      "peace 136\n",
      "wayne 136\n",
      "quick 136\n",
      "kanye 136\n",
      "carry 135\n",
      "fake 135\n",
      "anymore 134\n",
      "everyday 134\n",
      "worth 134\n",
      "crying 134\n",
      "spot 134\n",
      "reasons 133\n",
      "paid 133\n",
      "fame 133\n",
      "yellow 132\n",
      "cup 132\n",
      "hang 132\n",
      "devil 132\n",
      "west 131\n",
      "fat 131\n",
      "funny 131\n",
      "grind 131\n",
      "birthday 131\n",
      "small 130\n",
      "workin 130\n",
      "knees 130\n",
      "needs 129\n",
      "cat 129\n",
      "telling 129\n",
      "booty 129\n",
      "shady 129\n",
      "mary 129\n",
      "leaving 128\n",
      "means 128\n",
      "dress 128\n",
      "burning 128\n",
      "shout 128\n",
      "magic 128\n",
      "benz 128\n",
      "miles 127\n",
      "asked 127\n",
      "monster 127\n",
      "nowhere 126\n",
      "doo 126\n",
      "wet 126\n",
      "teeth 126\n",
      "bottom 126\n",
      "bobby 126\n",
      "watching 125\n",
      "tear 125\n",
      "warm 125\n",
      "clear 125\n",
      "box 125\n",
      "faith 125\n",
      "strange 124\n",
      "rather 124\n",
      "near 124\n",
      "yesterday 124\n",
      "knock 124\n",
      "hee 124\n",
      "boss 124\n",
      "angel 123\n",
      "known 123\n",
      "f 123\n",
      "icky 123\n",
      "hour 122\n",
      "answer 122\n",
      "clouds 122\n",
      "takin 122\n",
      "glad 122\n",
      "kid 121\n",
      "bye 121\n",
      "wife 121\n",
      "sell 121\n",
      "yeezy 121\n",
      "songs 120\n",
      "changes 120\n",
      "park 120\n",
      "mess 120\n",
      "cake 120\n",
      "seven 119\n",
      "grow 119\n",
      "jeans 119\n",
      "stuck 118\n",
      "ways 118\n",
      "middle 118\n",
      "point 118\n",
      "uhh 118\n",
      "empty 117\n",
      "single 117\n",
      "walked 117\n",
      "chain 117\n",
      "played 117\n",
      "k 117\n",
      "bird 116\n",
      "none 116\n",
      "versace 116\n",
      "sight 115\n",
      "type 115\n",
      "clean 115\n",
      "cried 115\n",
      "hop 115\n",
      "weak 115\n",
      "shots 115\n",
      "finally 115\n",
      "mi 115\n",
      "rolling 114\n",
      "sister 114\n",
      "plus 114\n",
      "news 113\n",
      "job 113\n",
      "lit 113\n",
      "john 113\n",
      "south 113\n",
      "mommy 113\n",
      "team 113\n",
      "khaled 113\n",
      "sorrow 112\n",
      "tongue 112\n",
      "fact 112\n",
      "quit 112\n",
      "fish 112\n",
      "blind 112\n",
      "handle 112\n",
      "band 111\n",
      "wine 111\n",
      "daughter 111\n",
      "secret 111\n",
      "motherfuckin 111\n",
      "mom 110\n",
      "fix 110\n",
      "movie 110\n",
      "needed 110\n",
      "unless 110\n",
      "hours 110\n",
      "hearts 110\n",
      "fingers 110\n",
      "playin 110\n",
      "christmas 109\n",
      "dry 109\n",
      "soft 109\n",
      "heat 109\n",
      "later 108\n",
      "deal 108\n",
      "return 108\n",
      "ahead 107\n",
      "changed 107\n",
      "haters 107\n",
      "rapper 107\n",
      "grown 106\n",
      "dope 106\n",
      "race 106\n",
      "hook 106\n",
      "chest 105\n",
      "less 105\n",
      "brought 105\n",
      "mountain 105\n",
      "vibe 105\n",
      "bar 105\n",
      "hoo 105\n",
      "dumb 104\n",
      "radio 104\n",
      "suck 104\n",
      "holding 104\n",
      "table 104\n",
      "price 104\n",
      "twice 104\n",
      "thee 104\n",
      "storm 104\n",
      "longer 104\n",
      "loud 103\n",
      "lives 103\n",
      "que 103\n",
      "mo 103\n",
      "birds 102\n",
      "realize 102\n",
      "scene 102\n",
      "country 102\n",
      "spirit 102\n",
      "hallelujah 102\n",
      "sent 102\n",
      "stuff 102\n",
      "ta 102\n",
      "poor 101\n",
      "v 101\n",
      "blues 101\n",
      "double 101\n",
      "chains 101\n",
      "dig 101\n",
      "feelings 100\n",
      "share 100\n",
      "crowd 100\n",
      "slim 100\n",
      "aye 100\n",
      "pretend 99\n",
      "forgive 99\n",
      "grave 99\n",
      "killed 99\n",
      "count 99\n",
      "shining 99\n",
      "tellin 99\n",
      "guns 99\n",
      "angels 99\n",
      "sunshine 99\n",
      "harder 99\n",
      "weezy 99\n",
      "loose 98\n",
      "driving 98\n",
      "class 98\n",
      "extra 98\n",
      "married 98\n",
      "happen 98\n",
      "crew 98\n",
      "expect 98\n",
      "amy 98\n",
      "waste 97\n",
      "lead 97\n",
      "shame 97\n",
      "rings 97\n",
      "hungry 97\n",
      "fit 97\n",
      "funky 97\n",
      "purple 97\n",
      "prince 97\n",
      "week 97\n",
      "heavy 96\n",
      "cops 96\n",
      "wave 96\n",
      "rush 96\n",
      "woah 96\n",
      "question 96\n",
      "sitting 96\n",
      "sir 96\n",
      "cream 96\n",
      "anyone 95\n",
      "ends 95\n",
      "state 95\n",
      "flowers 95\n",
      "silver 95\n",
      "crib 95\n",
      "switch 95\n",
      "notorious 95\n",
      "id 94\n",
      "sense 94\n",
      "figure 94\n",
      "walkin 94\n",
      "smell 94\n",
      "trouble 94\n",
      "wide 93\n",
      "wrote 93\n",
      "clock 93\n",
      "spent 93\n",
      "blessed 93\n",
      "sunday 93\n",
      "locked 93\n",
      "short 92\n",
      "bite 92\n",
      "pictures 92\n",
      "happened 92\n",
      "learned 92\n",
      "safe 92\n",
      "dollar 92\n",
      "special 92\n",
      "places 92\n",
      "poppin 92\n",
      "nose 92\n",
      "bom 92\n",
      "ive 92\n",
      "choose 91\n",
      "rule 91\n",
      "hanging 91\n",
      "isn 91\n",
      "anyway 91\n",
      "doubt 91\n",
      "fighting 91\n",
      "ur 91\n",
      "sleeping 90\n",
      "rules 90\n",
      "steady 90\n",
      "j 90\n",
      "bullshit 90\n",
      "neck 90\n",
      "problem 89\n",
      "cover 89\n",
      "lucky 89\n",
      "smokin 89\n",
      "snow 89\n",
      "church 89\n",
      "beef 89\n",
      "worst 88\n",
      "trip 88\n",
      "begin 88\n",
      "instead 88\n",
      "glass 88\n",
      "winter 88\n",
      "fo 88\n",
      "aah 88\n",
      "dem 88\n",
      "couldnt 88\n",
      "arm 87\n",
      "problems 87\n",
      "store 87\n",
      "ago 87\n",
      "pink 87\n",
      "tall 87\n",
      "pocket 87\n",
      "flip 87\n",
      "louis 87\n",
      "theres 87\n",
      "kept 86\n",
      "faces 86\n",
      "moving 86\n",
      "working 86\n",
      "bottles 86\n",
      "mercy 86\n",
      "belong 86\n",
      "thou 86\n",
      "legs 86\n",
      "trees 86\n",
      "rose 86\n",
      "coke 86\n",
      "rat 86\n",
      "pack 86\n",
      "dirt 85\n",
      "desire 85\n",
      "happiness 85\n",
      "early 85\n",
      "plane 85\n",
      "flying 85\n",
      "er 85\n",
      "marry 85\n",
      "girlfriend 85\n",
      "holdin 85\n",
      "jay 85\n",
      "limit 85\n",
      "american 85\n",
      "greatest 84\n",
      "speed 84\n",
      "rollin 84\n",
      "st 84\n",
      "except 84\n",
      "cali 84\n",
      "distance 83\n",
      "attention 83\n",
      "having 83\n",
      "lean 83\n",
      "hill 83\n",
      "drama 83\n",
      "paradise 83\n",
      "uptown 83\n",
      "shimmy 83\n",
      "radar 83\n",
      "dying 82\n",
      "pants 82\n",
      "tv 82\n",
      "plan 82\n",
      "ship 82\n",
      "self 82\n",
      "riding 82\n",
      "grass 82\n",
      "respect 82\n",
      "rockin 82\n",
      "prove 82\n",
      "thunder 82\n",
      "dah 82\n",
      "major 82\n",
      "seat 81\n",
      "letter 81\n",
      "thoughts 81\n",
      "leaves 81\n",
      "plans 81\n",
      "beauty 81\n",
      "given 81\n",
      "darlin 81\n",
      "zone 81\n",
      "sold 81\n",
      "bells 81\n",
      "jammin 81\n",
      "cock 80\n",
      "drugs 80\n",
      "closer 80\n",
      "nine 80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f0070e836381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcnt_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcnt_r\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcountword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnt_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f0070e836381>\u001b[0m in \u001b[0;36mcountword\u001b[0;34m(word, cor)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mspi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mspi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4) word frequency caculation\n",
    "def countword(word,cor):\n",
    "    splitted = cor.split(' ')\n",
    "    cnt = 0\n",
    "    for spi in splitted:\n",
    "        if word == spi:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "for word in word_list.values():\n",
    "    cnt_r = 0\n",
    "    for cor in corpus:\n",
    "        cnt_r += countword(word,cor)\n",
    "    print(word,cnt_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "clean-robinson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124634, 14)\n",
      "[  2   4 380  16 251  10 151   7  15   3   0   0   0   0]\n",
      "[  4 380  16 251  10 151   7  15   3   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,tgt_input,test_size=0.2,random_state=7)\n",
    "print(enc_train.shape)\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-stability",
   "metadata": {},
   "source": [
    "# making data obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "integrated-token",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-retirement",
   "metadata": {},
   "source": [
    "# Making NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "alpine-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size_256 = 256\n",
    "embedding_size_512 = 512\n",
    "embedding_size_1024 = 1024\n",
    "\n",
    "hidden_size_1024 = 1024\n",
    "\n",
    "model_256 = TextGenerator(tokenizer.num_words + 1, embedding_size_256 , hidden_size_1024)\n",
    "model_512 = TextGenerator(tokenizer.num_words + 1, embedding_size_512 , hidden_size_1024)\n",
    "model_1024 = TextGenerator(tokenizer.num_words + 1, embedding_size_1024 , hidden_size_1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "blocked-stand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-3.4075015e-04,  6.3131127e-05,  3.1429663e-04, ...,\n",
       "         -3.3353126e-04,  3.7346571e-04, -1.7741224e-04],\n",
       "        [-1.5780328e-05,  5.9107144e-04,  4.9672404e-04, ...,\n",
       "         -5.5604958e-04,  2.8807513e-04, -3.0980541e-04],\n",
       "        [ 5.8825733e-04,  5.4587051e-04,  8.5185032e-04, ...,\n",
       "          3.3580576e-04,  8.3170235e-05,  7.2784816e-05],\n",
       "        ...,\n",
       "        [-7.5425072e-05, -3.8450971e-04,  7.2748640e-05, ...,\n",
       "          1.1259964e-03, -8.8564947e-04,  1.7225716e-03],\n",
       "        [ 4.1206953e-05, -1.0404268e-03, -1.7760563e-04, ...,\n",
       "          9.1674284e-04, -8.9251599e-04,  1.1176531e-03],\n",
       "        [-8.2374696e-05, -1.5279116e-03, -3.3882435e-04, ...,\n",
       "          5.3805608e-04, -1.2416464e-03,  2.3276308e-04]],\n",
       "\n",
       "       [[-3.4075015e-04,  6.3131127e-05,  3.1429663e-04, ...,\n",
       "         -3.3353126e-04,  3.7346571e-04, -1.7741224e-04],\n",
       "        [-9.7924785e-04, -2.0659184e-04,  5.5647304e-04, ...,\n",
       "         -6.4967055e-04,  8.7525253e-04, -8.0320227e-04],\n",
       "        [-1.0329882e-03, -6.7774142e-04,  4.5202440e-04, ...,\n",
       "         -2.8547409e-04,  1.5198726e-03, -1.5886604e-03],\n",
       "        ...,\n",
       "        [ 8.0553745e-04, -2.0981722e-03, -9.4511925e-04, ...,\n",
       "          7.7973172e-04, -2.3906156e-03, -6.9349608e-03],\n",
       "        [ 1.1760871e-03, -2.1015857e-03, -1.1664707e-03, ...,\n",
       "          5.0457154e-04, -3.0981125e-03, -7.4418271e-03],\n",
       "        [ 1.6056387e-03, -2.1039303e-03, -1.3810151e-03, ...,\n",
       "          2.4027348e-04, -3.7295045e-03, -7.8708390e-03]],\n",
       "\n",
       "       [[-3.4075015e-04,  6.3131127e-05,  3.1429663e-04, ...,\n",
       "         -3.3353126e-04,  3.7346571e-04, -1.7741224e-04],\n",
       "        [-4.2153592e-04,  4.9101340e-04,  8.7801245e-04, ...,\n",
       "         -6.1281654e-04,  1.7622365e-04, -4.3217337e-04],\n",
       "        [-5.1764899e-04,  8.3721970e-04,  8.1339263e-04, ...,\n",
       "         -5.8860512e-04,  3.1536422e-04, -8.3270681e-04],\n",
       "        ...,\n",
       "        [ 2.2815962e-03, -1.5149316e-03, -7.4434275e-04, ...,\n",
       "          1.2803334e-05, -2.4378058e-03, -5.0195614e-03],\n",
       "        [ 2.6038380e-03, -1.7985016e-03, -1.0256965e-03, ...,\n",
       "         -1.0235025e-04, -3.1043848e-03, -5.7033980e-03],\n",
       "        [ 2.9582081e-03, -2.0093541e-03, -1.2890708e-03, ...,\n",
       "         -2.2157715e-04, -3.7191552e-03, -6.3185478e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.4075015e-04,  6.3131127e-05,  3.1429663e-04, ...,\n",
       "         -3.3353126e-04,  3.7346571e-04, -1.7741224e-04],\n",
       "        [-5.3818524e-04, -1.2750909e-04,  1.9390332e-04, ...,\n",
       "         -6.1334838e-04,  6.7645247e-04, -8.2909122e-05],\n",
       "        [-5.6857517e-04, -1.4319348e-05,  5.9406029e-04, ...,\n",
       "         -7.6044828e-04,  6.4566213e-04, -4.3275621e-04],\n",
       "        ...,\n",
       "        [ 4.6429825e-03, -5.5182958e-03, -1.2919774e-04, ...,\n",
       "         -1.7566534e-03, -6.6667871e-04, -4.3378435e-03],\n",
       "        [ 4.5133652e-03, -5.1769698e-03, -3.1710655e-04, ...,\n",
       "         -1.7909905e-03, -1.5883183e-03, -4.9541323e-03],\n",
       "        [ 4.4583497e-03, -4.8052259e-03, -5.5485696e-04, ...,\n",
       "         -1.7767384e-03, -2.4418819e-03, -5.5286167e-03]],\n",
       "\n",
       "       [[-3.4075015e-04,  6.3131127e-05,  3.1429663e-04, ...,\n",
       "         -3.3353126e-04,  3.7346571e-04, -1.7741224e-04],\n",
       "        [-5.9551472e-04,  1.4107290e-04,  8.9530699e-04, ...,\n",
       "         -3.5372158e-04,  7.8386866e-04, -5.8156918e-05],\n",
       "        [-6.6407677e-04,  1.2632927e-04,  1.1346646e-03, ...,\n",
       "         -4.0715185e-04,  1.0214216e-03, -3.3763936e-04],\n",
       "        ...,\n",
       "        [ 1.9295150e-03, -2.5171670e-03, -9.1935927e-04, ...,\n",
       "         -9.6361985e-04, -3.9502787e-03, -7.0958566e-03],\n",
       "        [ 2.4309792e-03, -2.5510855e-03, -1.2160850e-03, ...,\n",
       "         -1.0555144e-03, -4.4687367e-03, -7.5198379e-03],\n",
       "        [ 2.9015010e-03, -2.5625722e-03, -1.4830121e-03, ...,\n",
       "         -1.1489515e-03, -4.9113203e-03, -7.8782961e-03]],\n",
       "\n",
       "       [[-3.4075015e-04,  6.3131127e-05,  3.1429663e-04, ...,\n",
       "         -3.3353126e-04,  3.7346571e-04, -1.7741224e-04],\n",
       "        [-5.4627622e-04,  4.6351881e-04,  4.6209455e-04, ...,\n",
       "         -6.9517462e-04,  7.6415745e-04, -1.1601892e-04],\n",
       "        [-5.4270279e-04,  9.3765871e-04,  7.1962224e-04, ...,\n",
       "         -9.4501377e-04,  1.2850106e-03, -8.4623513e-05],\n",
       "        ...,\n",
       "        [-4.0355657e-04, -1.6159548e-03,  1.1629980e-03, ...,\n",
       "         -2.5715053e-04,  9.6438234e-05, -2.2521254e-03],\n",
       "        [-1.1964968e-04, -2.0128032e-03,  6.3298183e-04, ...,\n",
       "         -3.0814792e-04, -3.9294301e-04, -3.2077122e-03],\n",
       "        [ 2.5424093e-04, -2.2912808e-03,  1.0142027e-04, ...,\n",
       "         -3.5892014e-04, -1.0027933e-03, -4.1043628e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model_256(src_sample)\n",
    "model_512(src_sample)\n",
    "model_1024(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-active",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "thick-quantum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "486/486 [==============================] - 163s 329ms/step - loss: 4.0523\n",
      "Epoch 2/10\n",
      "486/486 [==============================] - 160s 329ms/step - loss: 3.0661\n",
      "Epoch 3/10\n",
      "486/486 [==============================] - 159s 327ms/step - loss: 2.8847\n",
      "Epoch 4/10\n",
      "486/486 [==============================] - 159s 326ms/step - loss: 2.7488\n",
      "Epoch 5/10\n",
      "486/486 [==============================] - 162s 333ms/step - loss: 2.6363\n",
      "Epoch 6/10\n",
      "486/486 [==============================] - 159s 326ms/step - loss: 2.5385\n",
      "Epoch 7/10\n",
      "486/486 [==============================] - 159s 327ms/step - loss: 2.4552\n",
      "Epoch 8/10\n",
      "486/486 [==============================] - 159s 326ms/step - loss: 2.3720\n",
      "Epoch 9/10\n",
      "486/486 [==============================] - 159s 328ms/step - loss: 2.2977\n",
      "Epoch 10/10\n",
      "486/486 [==============================] - 159s 327ms/step - loss: 2.2213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d5888a550>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model_256.compile(loss=loss, optimizer=optimizer)\n",
    "model_256.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "responsible-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "486/486 [==============================] - 171s 347ms/step - loss: 3.9029\n",
      "Epoch 2/10\n",
      "486/486 [==============================] - 169s 348ms/step - loss: 3.0658\n",
      "Epoch 3/10\n",
      "486/486 [==============================] - 170s 349ms/step - loss: 2.8941\n",
      "Epoch 4/10\n",
      "486/486 [==============================] - 170s 349ms/step - loss: 2.7732\n",
      "Epoch 5/10\n",
      "486/486 [==============================] - 170s 349ms/step - loss: 2.6491\n",
      "Epoch 6/10\n",
      "486/486 [==============================] - 170s 349ms/step - loss: 2.5280\n",
      "Epoch 7/10\n",
      "486/486 [==============================] - 175s 359ms/step - loss: 2.4210\n",
      "Epoch 8/10\n",
      "486/486 [==============================] - 170s 349ms/step - loss: 2.3081\n",
      "Epoch 9/10\n",
      "486/486 [==============================] - 170s 349ms/step - loss: 2.1961\n",
      "Epoch 10/10\n",
      "486/486 [==============================] - 170s 349ms/step - loss: 2.0894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d596eb150>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_512.compile(loss=loss, optimizer=optimizer)\n",
    "model_512.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "facial-island",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "486/486 [==============================] - 202s 409ms/step - loss: 3.9199\n",
      "Epoch 2/10\n",
      "486/486 [==============================] - 199s 409ms/step - loss: 3.1663\n",
      "Epoch 3/10\n",
      "486/486 [==============================] - 199s 410ms/step - loss: 3.0049\n",
      "Epoch 4/10\n",
      "486/486 [==============================] - 199s 410ms/step - loss: 2.9197\n",
      "Epoch 5/10\n",
      "486/486 [==============================] - 200s 410ms/step - loss: 2.8563\n",
      "Epoch 6/10\n",
      "486/486 [==============================] - 199s 410ms/step - loss: 2.8114\n",
      "Epoch 7/10\n",
      "486/486 [==============================] - 200s 410ms/step - loss: 2.7601\n",
      "Epoch 8/10\n",
      "486/486 [==============================] - 200s 410ms/step - loss: 2.7216\n",
      "Epoch 9/10\n",
      "486/486 [==============================] - 199s 410ms/step - loss: 2.6764\n",
      "Epoch 10/10\n",
      "486/486 [==============================] - 200s 411ms/step - loss: 2.6361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d53733190>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1024.compile(loss=loss, optimizer=optimizer)\n",
    "model_1024.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-worcester",
   "metadata": {},
   "source": [
    "# generate text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "classical-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adapted-certification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she s got me runnin round and round <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model_256, tokenizer, init_sentence=\"<start> she\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "approved-companion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she s got me runnin round and round <end> '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model_512, tokenizer, init_sentence=\"<start> she\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "stopped-brake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she s a <unk> <end> '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model_1024, tokenizer, init_sentence=\"<start> she\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-spectacular",
   "metadata": {},
   "source": [
    "# overview\n",
    "\t기계는 사람과 친구처럼 대화할 수 있을까? 매일 터미널 앞에서 혹은 코랩에 마주할 때면 기계의 생각을 읽고 싶을 때가 많다. “내가 명령어를 만들어 주면 너는 왜 항상 불평과 불만을 주는것이냐?, 너는 내가 하는 말들을 왜 못알아 먹는 것이냐?” 기계는 3살짜리 어린아이를 달래주는 것과 비슷하다. 기계를 설득하기 위해서 전적으로 나의 행동들이 모두 잘못된 것이다. 들여쓰기 하나 못한 것도 기계의 입장에서는 중대한 대역죄로 취급된다. 나의 행동들을 교정하면서 기계를 설득하다 보면 녹초가 되곤 한다. 하지만 오늘의 NLP모델은 미래에 컴퓨터와의 대화에서 이러한 스트레스에 자유를 줄지도 모른다는 생각이 든다. 사람의 문장들을 학습시켜 적절한 문장을 구사하는 아주 기초적인 실습이지만 기게와의 대화는 이제까지 사람의 명령에 묵묵히 수행했던 답답하고 투박한 상자에서 그들의 생각을 물어볼 수 있는 기술로 기대된다. \n",
    "\t이 실습에서의 목적은 embedding과 lstm의 rnn의 알고리즘을 이용하여 단어들을 학습시키고 하나의 단어를 입력하였을 때 적절한 문장을 출력하는 것이다. 이 실습에서 embedding의 기능과 하이퍼파라마미터를 변경하면서 학습 성능을 평가할 수 있는 loss의 값의 추이를 알아볼 것이며 2.2 이하로 낮추도록 설계해 볼 것이다.\n",
    "\n",
    "# Design\n",
    "\n",
    "\taiffel에서 제공한 각종 팝송의 가사들을 데이터를 입력시키고 각 가사들의 단어들을 적절한 벡터와 토큰화시키는 전처리(preprocessing)작업을 수행한다. 그리고 단어들의 유사성을 계산하고 표현하기 위해 one-hot encoding대신 embdding 기술을 이용하며 TFmaster에서 배운 LTSM으로 이전의 처리된 정보들을 참조하여 단어들 간의 유사성을 계산한다. \n",
    "\n",
    "# Analysis\n",
    "\t전처리 과정에서 lms에서 제공한 전처리 함수를 그대로 이용하였다. 불필요한 특수문자들을 제거하고 일련의 NLPmodel을 형성하였다. 이 때 모델의 loss(epoch=10)는 2.42로 2.2이하의 목표에 미치지 못했다. 하지만 토큰의 갯수를 15개로 제한하였을 때 모델의 loss는 2.2213으로 성능 개선이 이루어졌다. 이는 cs231에서 배운 backpropagation으로 gradient를 계산하지 않아도 입력값의 변화는 2.42-2.2213 = (about) 0.2의 차이를 보임을 확인할 수 있다. 이번에는 embdding size를 변화해보기로 하였다. 256, 512, 1024로 2배씩 늘려 model_256, model_512, model_1024로 나누어 성능을 평가해보았다. \n",
    "\n",
    "emodding size/time/loss : \t256/160s/2.422\n",
    "\t\t\t\t\t\t512/170s/2.0894\n",
    "\t\t\t\t\t\t1024/200s/2.6361\n",
    "\n",
    "이러한 embedding size의 변화는 loss의 변화를 나태냄이 뚜렷하였다. embdding size가 512일 때 가장 성능이 좋았고 1024일 때 가장 최악의 성능을 보였다. 이는 임베딩 사이즈가 클수록 좋은 것이 아닌 특정 최적의 지점이 있음을 내포하고 있다. 이를 구체적으로 알아보기 위해선 embedding size의 범위를 충분히 확보하고 적절한 간격으로 엠베딩 사이즈를 확장하여 성능의 변화를 확인하여야 한다. 하지만 이는 매우 비용이 큰 작업이다. 에폭 10을 모두 소화하는데 무려 30분이나 걸린다. 따라서 임베딩 사이즈는 무턱대고 일일이 값들을 설정하여 최적의 사이즈를 찾는 것은 효율적이지 못하다. \n",
    "\n",
    "\t그리고 각 모델에 “she”를 입력해보았다. model_256과 model_512는 “<start> she s got me runnin round and round <end>”라는 다소 어색하지만 그럴듯한 문장을 만들어내었다. 반면 model_1024는 “<start> she s a <unk> <end>”라는 문장을 만들어냈으며 문장을 모두 완성해내지 못했다. 이 모델은 loss가 2.6361로 가장 성능이 저조하였고 이정도의 loss는 문장하나를 만들어내는 적합한 모델을 만들지 못한다는 것을 알 수 있다. \n",
    "\n",
    "\n",
    "# Limits\n",
    "\t히든 사이즈의 변화에 따라 어떠한 loss의 변화를 보이는지 확인해보아야 했다.\n",
    "\t최적의 embedding size를 찾는 방법에 대해 강구했어야 했다.\n",
    "\t어색한 문장에 대해 원인을 파악하고 해결했어야 했다. (she s got me runnin round and round는 실제 사람의 대화에서 표현하지 않는다. )\n",
    "\t\n",
    "# In my opinion\n",
    "\t(이 글은 학습시키는데 너무 무료한 나머지 개인적인 생각을 적은 것들입니다. 무료함 속에서 자유로운 생각과 상상의 나래를 펼칠 수 있는 좋은 경험인것 같습니다.)\n",
    "\n",
    "\n",
    "\tembedding 기술은 머신러닝에서 해당 카테고리값들을 숫자와 벡터로 변환하여 유사성과 표현효율성을 높이는데 매우 탁월한 면모를 보인다는 것은 매우 흥미롭다. LMS에서 배운 one hot encoding과 구글에서 얻은 정보들을 종합해보면 oen hot encdoing은 0과 1의 표현으로 카테고리가 늘어나면 필요한 벡터공간은 면적배로 늘어나는 단점을 지닌다. 해당 카테고리를 의미하는 1은 겨우 1개인데 나머지는 0으로 매핑되는 것은 데이터 공간의 비효율성이 한눈에 보인다. 더군다나 각 카테고리들의 유사성을 one hot encoding으로 표현하기 역부족이다. 기계는 유사성을 각 카테고리의 밸류의 거리에 대해서 인지할 수 있는데 one hot encoding은 각 카테고리의 거리들은 동일하다. (예를 들면 3차원 공간에서 (1,0,0)의 점과 (0,1,0)의 점 (0,0,1)의 세 점의 거리는 모두 동일하다)\n",
    "\t이러한 한계에 대해서 embedding 기술은 자연어의 단어들 간의 유사성을 표현하는데 매우 탁월한 기술임을 코딩을 통해 확인할 수 있었다. 모델을 만들고 하나의 배치사이즈의 데이터를 입력해보면 단어들의 encoding된 값들이 표현되며 이 값들을 토대로 단어들의 유사성을 직관적으로 확인할 수 없지만 결과 값들의 차이로 단어들 간의 유사성을 표현할 수 있음을 알 수 있다. \n",
    "\n",
    "\t하지만 나는 사람이 대화를 할 때의 반응속도에 대해서 생각해보았다. 나는 LMS를 하면서 팝송을 즐겨 듣는다. 그리고 오늘 들었던 팝송들 중 유난히 빠른 곡이 하나 있었다. jake miller의 rumors는 다소 빠른 속도의 비트와 함께 음악이 진행된다. 나는 영어가 서툴러 이러한 단어들을 모두 이해할 수 없지만 반면 기초적 교육 과정을 거친 미국인들은 아무렇지 않듯 이 노래들을 흥겨듣거나 부르기까지 한다. 영어에 익숙한 미국인들이 한국인보다 더 빠르게 영어 단어들을 이해하고 구사할 수 있는 이유는 학습된 경험의 양이 많기 때문이다. 경험에서 어떤 상황에서 어떤 단어들과 표현을 구사할 것인가에 대해서 패턴들을 축적해왔고 끊임없이 더 나은 표현의 데이터베이스를 구축해 왔을 것이다. 그리고 특정 상황에 닥칠 때 반사적으로, 무의식적으로 가장 적합한 단어와 표현을 만들어낸다. \n",
    "\t내가 했던 이 NLP model은 얼마나 빠르게 반응하여 문장들을 자연스럽게 구사할 수 있을까? 오늘 해본 embedding과 lstm은 단어들의 유사성을 모두 계산하고 특정 단어를 입력하면 가장 유사성이 높은 단어들을 출력하여 하나의 문장을 완성한다. 하지만 적절한 문장을 만드는데는 문장들의 데이터셋을 집어넣어 빈도와 유사성만으로는 사람과 같이 복잡하고 어려운 문장들을 구사하기 힘들것이다. 그리고 학습된 데이터의 수가 많은수록 계산해야 할 유사성은 더욱 더 복잡해지며 최종적으로 출력할 때의 계산량은 아마도 엄청난 시간과 비용을 지불해야 할 지 모른다.\n",
    "\t반면 사람은 적합한 단어들을 구사할 때 단어들의 유사성의 값들을 계산하고 값들을 일일이 확인하지 않는다. 우리가 문장을 구사할 때 계산기를 두드려가며 문장의 적합도를 계산하는 대신 감각에 의존한다. 아주 짧은 찰나의 시간동안 구사할 문장에 대해서 느낌으로 적합한지 그렇지 않은지 시뮬레이션 하며 문장 구성의 모든 과정은 감각 의존적이다. 그렇다면 기계도 복잡한 수식에 의존하는 것보다 생물학적 감각으로 문장 구사과정을 설계할 수 없을까? 두뇌의 뉴런과 시냅스의 화학반응과 같이 감각반응이 장착된다면 감각으로 불필요한 단어들을 한번에 문장 과정에 고려하지 않고 몇가지만 추려 빠르게 적합한 문장을 구사할 수 있지 않을까? 만약 이것이 가능하다면 매우 많은 데이터에 대해서 시간과 비용을 아낄 수 있을지 모른다. \n",
    "\n",
    "\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
