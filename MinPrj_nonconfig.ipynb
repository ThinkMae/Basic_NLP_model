{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "covered-ireland",
   "metadata": {},
   "source": [
    "# MinPrj_Lyrics\n",
    "\n",
    "date : 5 october 2021\n",
    "author : bae hueng myoung\n",
    "prj goals : \n",
    "    1)\n",
    "    2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "happy-philippines",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Can we forget about the things I said when I was drunk...', \"I didn't mean to call you that\", \"I can't remember what was said\"]\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fossil-cancer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/aiffel/aiffel/lyricist/data/blink-182.txt', '/aiffel/aiffel/lyricist/data/al-green.txt', '/aiffel/aiffel/lyricist/data/johnny-cash.txt', '/aiffel/aiffel/lyricist/data/r-kelly.txt', '/aiffel/aiffel/lyricist/data/nirvana.txt', '/aiffel/aiffel/lyricist/data/dickinson.txt', '/aiffel/aiffel/lyricist/data/michael-jackson.txt', '/aiffel/aiffel/lyricist/data/paul-simon.txt', '/aiffel/aiffel/lyricist/data/prince.txt', '/aiffel/aiffel/lyricist/data/nicki-minaj.txt', '/aiffel/aiffel/lyricist/data/adele.txt', '/aiffel/aiffel/lyricist/data/Lil_Wayne.txt', '/aiffel/aiffel/lyricist/data/lorde.txt', '/aiffel/aiffel/lyricist/data/dj-khaled.txt', '/aiffel/aiffel/lyricist/data/bob-marley.txt', '/aiffel/aiffel/lyricist/data/alicia-keys.txt', '/aiffel/aiffel/lyricist/data/janisjoplin.txt', '/aiffel/aiffel/lyricist/data/lady-gaga.txt', '/aiffel/aiffel/lyricist/data/dolly-parton.txt', '/aiffel/aiffel/lyricist/data/britney-spears.txt', '/aiffel/aiffel/lyricist/data/lin-manuel-miranda.txt', '/aiffel/aiffel/lyricist/data/lil-wayne.txt', '/aiffel/aiffel/lyricist/data/notorious_big.txt', '/aiffel/aiffel/lyricist/data/eminem.txt', '/aiffel/aiffel/lyricist/data/bjork.txt', '/aiffel/aiffel/lyricist/data/notorious-big.txt', '/aiffel/aiffel/lyricist/data/missy-elliott.txt', '/aiffel/aiffel/lyricist/data/kanye-west.txt', '/aiffel/aiffel/lyricist/data/bruno-mars.txt', '/aiffel/aiffel/lyricist/data/nickelback.txt', '/aiffel/aiffel/lyricist/data/ludacris.txt', '/aiffel/aiffel/lyricist/data/joni-mitchell.txt', '/aiffel/aiffel/lyricist/data/jimi-hendrix.txt', '/aiffel/aiffel/lyricist/data/patti-smith.txt', '/aiffel/aiffel/lyricist/data/Kanye_West.txt', '/aiffel/aiffel/lyricist/data/kanye.txt', '/aiffel/aiffel/lyricist/data/cake.txt', '/aiffel/aiffel/lyricist/data/nursery_rhymes.txt', '/aiffel/aiffel/lyricist/data/bieber.txt', '/aiffel/aiffel/lyricist/data/disney.txt', '/aiffel/aiffel/lyricist/data/drake.txt', '/aiffel/aiffel/lyricist/data/radiohead.txt', '/aiffel/aiffel/lyricist/data/amy-winehouse.txt', '/aiffel/aiffel/lyricist/data/rihanna.txt', '/aiffel/aiffel/lyricist/data/leonard-cohen.txt', '/aiffel/aiffel/lyricist/data/dr-seuss.txt', '/aiffel/aiffel/lyricist/data/bob-dylan.txt', '/aiffel/aiffel/lyricist/data/beatles.txt', '/aiffel/aiffel/lyricist/data/bruce-springsteen.txt']\n"
     ]
    }
   ],
   "source": [
    "print(txt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-catalyst",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "    1) 불필요한 문자 제거\n",
    "    2) length < 13 : 토큰화 갯수가 15개 이하를 만족하기 위해선 각 문장의 공백은 13개 이하가 되어야 함\n",
    "    3) generating token\n",
    "    4) word frequency caculation / 입려된 문장들의 단어들이 얼마나의 빈도를 갖는지 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "moderate-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 불필요한 문자 제거\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "million-logging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187088\n",
      "174144\n",
      "['Can we forget about the things I said when I was drunk...', \"I didn't mean to call you that\", \"I can't remember what was said\", 'Or what you threw at me Please tell me', 'Please tell me why', 'My car is in the front yard', 'And I am sleeping with my cloths on', 'I came in throught the window... Last night', 'And your... Gone', \"Gone It's no suprise to me I am my own worst enemy\"]\n"
     ]
    }
   ],
   "source": [
    "# 2) length < 13 : 토큰화 갯수가 15개 이하를 만족하기 위해선 각 문장의 공백은 13개 이하가 되어야 함\n",
    "def blank_num_calc(sentence):\n",
    "    cnt = 0\n",
    "    for ele in sentence:\n",
    "        if ele == ' ':\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "    \n",
    "    \n",
    "lenmodcorp = []\n",
    "for sen in raw_corpus:\n",
    "    if blank_num_calc(sen) < 13:\n",
    "        lenmodcorp.append(sen)\n",
    "\n",
    "print(len(raw_corpus))\n",
    "print(len(lenmodcorp))\n",
    "print(lenmodcorp[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "surprised-mention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> can we forget about the things i said when i was drunk . . . <end>',\n",
       " '<start> i didn t mean to call you that <end>',\n",
       " '<start> i can t remember what was said <end>',\n",
       " '<start> or what you threw at me please tell me <end>',\n",
       " '<start> please tell me why <end>',\n",
       " '<start> my car is in the front yard <end>',\n",
       " '<start> and i am sleeping with my cloths on <end>',\n",
       " '<start> i came in throught the window . . . last night <end>',\n",
       " '<start> and your . . . gone <end>',\n",
       " '<start> gone it s no suprise to me i am my own worst enemy <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence_ele in lenmodcorp:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence_ele) == 0: continue\n",
    "    if sentence_ele[-1] == \":\": continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence_ele)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "robust-parking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162813, 14)\n",
      "<keras_preprocessing.text.Tokenizer object at 0x7f2ef870bc10>\n"
     ]
    }
   ],
   "source": [
    "# 3) generating token\n",
    "# making num_words = 12000\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "tensor = tensor[:,:14]\n",
    "print(tensor.shape)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "integral-finance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "word_list = tokenizer.index_word\n",
    "\n",
    "for idx in word_list:\n",
    "    print(idx, \":\", word_list[idx])\n",
    "    if idx >= 10: break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "placed-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> 0\n",
      "<start> 162813\n",
      "<end> 162813\n",
      ", 53789\n",
      "i 50661\n",
      "the 41496\n",
      "you 38345\n",
      "and 24463\n",
      "a 22205\n",
      "to 21802\n",
      "it 18449\n",
      "me 17212\n",
      "my 16256\n",
      "in 14029\n",
      "t 13264\n",
      "s 12974\n",
      "that 12554\n",
      "on 10488\n",
      ". 9697\n",
      "your 9576\n",
      "of 9511\n",
      "we 9146\n",
      "m 9102\n",
      "like 8301\n",
      "all 8101\n",
      "is 7699\n",
      "be 7205\n",
      "for 6928\n",
      "up 6837\n",
      "with 6631\n",
      "so 6560\n",
      "can 6233\n",
      "but 6219\n",
      "know 6216\n",
      "just 6211\n",
      "don 6163\n",
      "love 6017\n",
      "no 5874\n",
      "what 5723\n",
      "got 5714\n",
      "oh 5568\n",
      "they 5530\n",
      "this 5484\n",
      "? 5455\n",
      "get 5346\n",
      "she 5138\n",
      "when 5065\n",
      "do 4888\n",
      "yeah 4710\n",
      "now 4374\n",
      "baby 4314\n",
      "re 4294\n",
      "if 4257\n",
      "go 4134\n",
      "he 4115\n",
      "out 4036\n",
      "was 3962\n",
      "! 3845\n",
      "down 3755\n",
      "one 3635\n",
      "ll 3509\n",
      "there 3271\n",
      "see 3185\n",
      "let 3177\n",
      "want 3129\n",
      "cause 3099\n",
      "come 3093\n",
      "not 3066\n",
      "her 3042\n",
      "say 2990\n",
      "at 2962\n",
      "make 2943\n",
      "from 2798\n",
      "time 2793\n",
      "have 2742\n",
      "are 2720\n",
      "back 2711\n",
      "how 2609\n",
      "never 2555\n",
      "girl 2501\n",
      "im 2490\n",
      "take 2454\n",
      "as 2439\n",
      "man 2331\n",
      "will 2314\n",
      "right 2286\n",
      "way 2256\n",
      "wanna 2223\n",
      "u 2179\n",
      "ain 2157\n",
      "ve 2108\n",
      "here 2071\n",
      "need 2070\n",
      "then 2034\n",
      "who 2004\n",
      "tell 1998\n",
      "some 1956\n",
      "more 1925\n",
      "gonna 1925\n",
      "where 1877\n",
      "his 1821\n",
      "too 1819\n",
      "been 1789\n",
      "could 1771\n",
      "feel 1770\n",
      "them 1759\n",
      "life 1739\n",
      "or 1714\n",
      "good 1689\n",
      "shit 1664\n",
      "said 1638\n",
      "give 1637\n",
      "about 1626\n",
      "why 1626\n",
      "night 1604\n",
      "had 1604\n",
      "little 1586\n",
      "off 1585\n",
      "by 1565\n",
      "day 1555\n",
      "ya 1501\n",
      "only 1496\n",
      "keep 1465\n",
      "still 1458\n",
      "em 1451\n",
      "every 1447\n",
      "us 1444\n",
      "think 1440\n",
      "bitch 1426\n",
      "through 1413\n",
      "fuck 1411\n",
      "d 1402\n",
      "look 1393\n",
      "these 1356\n",
      "would 1342\n",
      "around 1342\n",
      "away 1337\n",
      "heart 1311\n",
      "hey 1309\n",
      "its 1295\n",
      "money 1291\n",
      "over 1274\n",
      "him 1265\n",
      "world 1262\n",
      "well 1256\n",
      "dont 1255\n",
      "gotta 1223\n",
      "la 1222\n",
      "better 1205\n",
      "put 1200\n",
      "an 1200\n",
      "our 1178\n",
      "call 1170\n",
      "new 1155\n",
      "am 1141\n",
      "bad 1120\n",
      "boy 1116\n",
      "even 1111\n",
      "ever 1073\n",
      "stop 1069\n",
      "aint 1060\n",
      "really 1057\n",
      "home 1054\n",
      "nigga 1047\n",
      "won 1045\n",
      "long 1034\n",
      "than 1022\n",
      "always 1000\n",
      "did 998\n",
      "big 989\n",
      "nothing 985\n",
      "niggas 981\n",
      "mind 971\n",
      "people 963\n",
      "things 958\n",
      "before 958\n",
      "everything 955\n",
      "were 949\n",
      "again 947\n",
      "everybody 937\n",
      "head 934\n",
      "ooh 933\n",
      "much 912\n",
      "hold 907\n",
      "thing 902\n",
      "show 894\n",
      "hear 884\n",
      "da 868\n",
      "god 864\n",
      "gone 857\n",
      "leave 855\n",
      "name 855\n",
      "yo 850\n",
      "ass 843\n",
      "should 839\n",
      "something 835\n",
      "eyes 829\n",
      "hard 826\n",
      "turn 825\n",
      "live 819\n",
      "going 808\n",
      "another 808\n",
      "real 800\n",
      "hit 800\n",
      "find 786\n",
      "face 781\n",
      "talk 780\n",
      "try 775\n",
      "old 759\n",
      "high 756\n",
      "play 755\n",
      "body 754\n",
      "uh 739\n",
      "light 731\n",
      "alone 725\n",
      "stay 718\n",
      "believe 717\n",
      "please 710\n",
      "two 709\n",
      "mine 702\n",
      "into 694\n",
      "made 693\n",
      "nixga 693\n",
      "dance 692\n",
      "na 691\n",
      "work 684\n",
      "told 680\n",
      "last 679\n",
      "tonight 678\n",
      "best 671\n",
      "hot 665\n",
      "hand 664\n",
      "their 660\n",
      "bout 657\n",
      "thats 655\n",
      "while 649\n",
      "other 649\n",
      "yes 634\n",
      "nobody 632\n",
      "black 619\n",
      "gon 617\n",
      "left 614\n",
      "ah 613\n",
      "came 611\n",
      "round 606\n",
      "die 604\n",
      "cry 601\n",
      "mean 598\n",
      "bitches 596\n",
      "somebody 595\n",
      "fly 592\n",
      "chorus 590\n",
      "place 588\n",
      "wait 587\n",
      "run 586\n",
      "first 582\n",
      "without 582\n",
      "stand 582\n",
      "enough 578\n",
      "whole 578\n",
      "done 576\n",
      "free 569\n",
      "girls 567\n",
      "sky 563\n",
      "friends 562\n",
      "hands 557\n",
      "young 556\n",
      "alright 552\n",
      "same 551\n",
      "cant 550\n",
      "wrong 547\n",
      "thought 542\n",
      "has 540\n",
      "damn 539\n",
      "crazy 537\n",
      "must 532\n",
      "together 528\n",
      "wish 523\n",
      "inside 523\n",
      "whoa 520\n",
      "maybe 517\n",
      "game 515\n",
      "coming 514\n",
      "care 513\n",
      "own 512\n",
      "fire 509\n",
      "goin 505\n",
      "went 502\n",
      "til 498\n",
      "ready 496\n",
      "rock 493\n",
      "looking 492\n",
      "fall 483\n",
      "party 483\n",
      "y 483\n",
      "break 481\n",
      "sun 480\n",
      "used 480\n",
      "move 477\n",
      "touch 473\n",
      "myself 468\n",
      "many 462\n",
      "friend 462\n",
      "trying 460\n",
      "ride 459\n",
      "might 457\n",
      "heard 456\n",
      "ask 455\n",
      "soul 455\n",
      "remember 454\n",
      "bring 454\n",
      "knows 453\n",
      "because 451\n",
      "seen 451\n",
      "help 449\n",
      "end 448\n",
      "forever 448\n",
      "watch 447\n",
      "side 447\n",
      "sing 447\n",
      "guess 446\n",
      "lost 444\n",
      "door 444\n",
      "till 442\n",
      "fuckin 440\n",
      "someone 439\n",
      "white 437\n",
      "pussy 435\n",
      "though 434\n",
      "lie 433\n",
      "next 427\n",
      "wit 426\n",
      "house 425\n",
      "music 425\n",
      "mama 424\n",
      "feeling 423\n",
      "walk 423\n",
      "change 420\n",
      "beat 417\n",
      "true 415\n",
      "cold 412\n",
      "start 409\n",
      "goes 408\n",
      "dream 407\n",
      "took 407\n",
      "matter 407\n",
      "ma 405\n",
      "hope 402\n",
      "morning 401\n",
      "sweet 398\n",
      "o 398\n",
      "days 397\n",
      "after 395\n",
      "lord 395\n",
      "far 394\n",
      "may 394\n",
      "getting 393\n",
      "huh 393\n",
      "floor 391\n",
      "didn 389\n",
      "kiss 388\n",
      "waiting 386\n",
      "nixgas 386\n",
      "song 385\n",
      "rain 385\n",
      "yall 383\n",
      "until 381\n",
      "blue 379\n",
      "room 378\n",
      "top 378\n",
      "miss 376\n",
      "honey 373\n",
      "woman 373\n",
      "sometimes 372\n",
      "town 371\n",
      "any 371\n",
      "knew 371\n",
      "since 370\n",
      "biggie 370\n",
      "smile 369\n",
      "happy 368\n",
      "blow 367\n",
      "daddy 367\n",
      "bed 364\n",
      "dreams 363\n",
      "shot 361\n",
      "lights 360\n",
      "hate 359\n",
      "beautiful 359\n",
      "babe 359\n",
      "gettin 358\n",
      "boys 357\n",
      "drop 356\n",
      "deep 354\n",
      "pretty 353\n",
      "comes 352\n",
      "hair 351\n",
      "shake 350\n",
      "each 350\n",
      "eh 349\n",
      "dead 347\n",
      "those 347\n",
      "under 346\n",
      "lose 346\n",
      "once 346\n",
      "dick 345\n",
      "lay 345\n",
      "three 344\n",
      "times 343\n",
      "heaven 342\n",
      "throw 342\n",
      "roll 341\n",
      "hell 338\n",
      "close 338\n",
      "easy 336\n",
      "hoes 335\n",
      "talking 334\n",
      "alive 333\n",
      "pain 333\n",
      "ill 333\n",
      "city 332\n",
      "late 331\n",
      "hurt 331\n",
      "fight 330\n",
      "living 329\n",
      "low 329\n",
      "full 328\n",
      "fucking 328\n",
      "gimme 328\n",
      "understand 327\n",
      "cut 324\n",
      "road 322\n",
      "forget 320\n",
      "ha 320\n",
      "boom 320\n",
      "straight 319\n",
      "lot 316\n",
      "pull 316\n",
      "club 315\n",
      "words 314\n",
      "car 312\n",
      "years 312\n",
      "slow 312\n",
      "yourself 312\n",
      "check 311\n",
      "phone 310\n",
      "line 309\n",
      "gave 308\n",
      "sure 308\n",
      "red 308\n",
      "pay 307\n",
      "nasty 307\n",
      "lonely 306\n",
      "dark 305\n",
      "sleep 305\n",
      "eat 304\n",
      "open 302\n",
      "feels 301\n",
      "arms 301\n",
      "behind 300\n",
      "streets 299\n",
      "doing 297\n",
      "feet 296\n",
      "buy 296\n",
      "lookin 296\n",
      "mother 294\n",
      "x 294\n",
      "anything 292\n",
      "talkin 291\n",
      "wild 290\n",
      "word 288\n",
      "c 288\n",
      "n 288\n",
      "drink 287\n",
      "makes 287\n",
      "sex 287\n",
      "yet 286\n",
      "water 286\n",
      "air 285\n",
      "nah 284\n",
      "says 282\n",
      "meet 280\n",
      "gold 280\n",
      "saw 280\n",
      "million 279\n",
      "else 279\n",
      "listen 278\n",
      "today 276\n",
      "such 275\n",
      "fast 275\n",
      "nothin 275\n",
      "found 274\n",
      "wonder 273\n",
      "lets 272\n",
      "saying 271\n",
      "king 268\n",
      "set 267\n",
      "lady 267\n",
      "tryin 266\n",
      "broken 265\n",
      "act 265\n",
      "ho 265\n",
      "very 264\n",
      "pop 264\n",
      "hood 262\n",
      "kill 261\n",
      "verse 261\n",
      "street 259\n",
      "loving 259\n",
      "cool 258\n",
      "lil 258\n",
      "thinking 257\n",
      "feelin 256\n",
      "tryna 256\n",
      "smoke 255\n",
      "mad 255\n",
      "star 254\n",
      "tried 253\n",
      "swear 252\n",
      "mma 251\n",
      "kind 250\n",
      "wanted 250\n",
      "comin 250\n",
      "upon 249\n",
      "both 249\n",
      "doin 248\n",
      "g 248\n",
      "met 247\n",
      "tears 247\n",
      "mouth 246\n",
      "tight 245\n",
      "blood 245\n",
      "dog 245\n",
      "past 244\n",
      "wake 244\n",
      "being 244\n",
      "pray 244\n",
      "hello 243\n",
      "moon 243\n",
      "between 242\n",
      "gun 242\n",
      "use 241\n",
      "sorry 241\n",
      "rap 241\n",
      "summer 239\n",
      "kids 239\n",
      "soon 239\n",
      "flow 238\n",
      "sea 237\n",
      "chance 236\n",
      "fine 236\n",
      "drive 236\n",
      "shine 235\n",
      "started 234\n",
      "child 234\n",
      "truth 233\n",
      "fun 233\n",
      "clothes 232\n",
      "broke 232\n",
      "pick 232\n",
      "motherfucker 232\n",
      "women 232\n",
      "wind 232\n",
      "youre 232\n",
      "strong 231\n",
      "sit 231\n",
      "school 231\n",
      "fool 231\n",
      "making 230\n",
      "b 230\n",
      "eye 229\n",
      "lover 229\n",
      "push 229\n",
      "half 228\n",
      "save 227\n",
      "ground 227\n",
      "motherfuckers 227\n",
      "pass 227\n",
      "control 227\n",
      "wall 226\n",
      "reason 226\n",
      "mr 226\n",
      "brother 226\n",
      "year 225\n",
      "seems 225\n",
      "nixgaz 225\n",
      "whatever 224\n",
      "death 223\n",
      "chick 223\n",
      "felt 222\n",
      "bag 222\n",
      "children 221\n",
      "power 221\n",
      "bright 219\n",
      "shall 218\n",
      "sound 218\n",
      "somethin 218\n",
      "front 216\n",
      "ay 216\n",
      "number 215\n",
      "couldn 214\n",
      "standing 213\n",
      "read 213\n",
      "four 213\n",
      "rest 212\n",
      "along 211\n",
      "taking 210\n",
      "step 209\n",
      "e 209\n",
      "part 207\n",
      "perfect 207\n",
      "burn 206\n",
      "tomorrow 206\n",
      "wants 206\n",
      "does 206\n",
      "send 206\n",
      "funk 206\n",
      "green 204\n",
      "son 204\n",
      "sick 203\n",
      "different 203\n",
      "darling 202\n",
      "bought 201\n",
      "l 201\n",
      "loved 201\n",
      "win 200\n",
      "catch 200\n",
      "already 199\n",
      "window 198\n",
      "okay 198\n",
      "gets 197\n",
      "caught 197\n",
      "thank 197\n",
      "rich 197\n",
      "jesus 196\n",
      "father 196\n",
      "r 196\n",
      "stars 195\n",
      "most 195\n",
      "sayin 195\n",
      "loves 194\n",
      "ice 194\n",
      "wont 194\n",
      "jack 193\n",
      "tired 193\n",
      "men 193\n",
      "block 193\n",
      "running 192\n",
      "bet 192\n",
      "apart 191\n",
      "doesn 191\n",
      "hundred 191\n",
      "earth 190\n",
      "trust 190\n",
      "grab 190\n",
      "five 190\n",
      "cash 190\n",
      "called 189\n",
      "stupid 189\n",
      "rise 189\n",
      "land 189\n",
      "keys 189\n",
      "whats 188\n",
      "train 186\n",
      "family 186\n",
      "lies 185\n",
      "seem 185\n",
      "afraid 185\n",
      "fell 184\n",
      "shut 184\n",
      "bit 183\n",
      "wouldn 182\n",
      "fear 181\n",
      "lips 181\n",
      "moment 181\n",
      "promise 181\n",
      "de 181\n",
      "story 180\n",
      "cuz 178\n",
      "ladies 178\n",
      "nice 177\n",
      "follow 177\n",
      "ring 177\n",
      "yea 177\n",
      "born 177\n",
      "war 176\n",
      "voice 175\n",
      "speak 175\n",
      "woo 175\n",
      "bang 175\n",
      "york 174\n",
      "which 174\n",
      "imma 174\n",
      "missy 174\n",
      "bust 173\n",
      "scream 172\n",
      "above 172\n",
      "bone 172\n",
      "dirty 172\n",
      "style 172\n",
      "yours 171\n",
      "river 170\n",
      "sexy 170\n",
      "brain 170\n",
      "holy 170\n",
      "wear 169\n",
      "everyone 169\n",
      "calling 169\n",
      "thousand 169\n",
      "space 168\n",
      "minute 167\n",
      "blame 167\n",
      "worry 167\n",
      "ok 167\n",
      "freak 167\n",
      "ball 167\n",
      "mon 166\n",
      "against 165\n",
      "taste 165\n",
      "wasn 165\n",
      "across 165\n",
      "takes 165\n",
      "spit 165\n",
      "couple 165\n",
      "diamonds 165\n",
      "laugh 164\n",
      "twenty 163\n",
      "six 163\n",
      "walking 163\n",
      "welcome 163\n",
      "bum 163\n",
      "joy 161\n",
      "spend 161\n",
      "drunk 160\n",
      "prayer 160\n",
      "sign 158\n",
      "higher 158\n",
      "shoot 158\n",
      "paper 158\n",
      "livin 157\n",
      "diamond 157\n",
      "hide 156\n",
      "brown 156\n",
      "scared 155\n",
      "future 155\n",
      "died 155\n",
      "cars 155\n",
      "looks 155\n",
      "ten 154\n",
      "cannot 154\n",
      "skin 154\n",
      "lovin 154\n",
      "everywhere 154\n",
      "p 154\n",
      "reach 153\n",
      "almost 153\n",
      "dj 153\n",
      "lift 152\n",
      "learn 152\n",
      "great 152\n",
      "treat 152\n",
      "ones 152\n",
      "crack 152\n",
      "shoes 151\n",
      "looked 151\n",
      "track 151\n",
      "kick 150\n",
      "tree 150\n",
      "meant 150\n",
      "turned 150\n",
      "weed 150\n",
      "yah 150\n",
      "naked 149\n",
      "write 149\n",
      "runnin 149\n",
      "momma 149\n",
      "fresh 149\n",
      "business 149\n",
      "breath 148\n",
      "picture 148\n",
      "outta 148\n",
      "makin 148\n",
      "second 147\n",
      "breathe 146\n",
      "sad 146\n",
      "michael 146\n",
      "probably 145\n",
      "kinda 145\n",
      "goodbye 145\n",
      "somewhere 145\n",
      "queen 144\n",
      "outside 143\n",
      "giving 143\n",
      "key 143\n",
      "stick 142\n",
      "least 142\n",
      "ye 142\n",
      "supposed 141\n",
      "singing 141\n",
      "favorite 141\n",
      "keeps 141\n",
      "bridge 141\n",
      "brand 141\n",
      "human 141\n",
      "mirror 140\n",
      "ran 140\n",
      "hoe 140\n",
      "nights 139\n",
      "fucked 139\n",
      "guy 139\n",
      "few 139\n",
      "falling 139\n",
      "fill 139\n",
      "thinkin 139\n",
      "jump 139\n",
      "cross 139\n",
      "homie 139\n",
      "dancing 139\n",
      "dear 138\n",
      "wings 138\n",
      "piece 138\n",
      "playing 137\n",
      "anybody 137\n",
      "stone 136\n",
      "callin 136\n",
      "peace 136\n",
      "wayne 136\n",
      "quick 136\n",
      "kanye 136\n",
      "carry 135\n",
      "fake 135\n",
      "anymore 134\n",
      "everyday 134\n",
      "worth 134\n",
      "crying 134\n",
      "spot 134\n",
      "reasons 133\n",
      "paid 133\n",
      "fame 133\n",
      "yellow 132\n",
      "cup 132\n",
      "hang 132\n",
      "devil 132\n",
      "west 131\n",
      "fat 131\n",
      "funny 131\n",
      "grind 131\n",
      "birthday 131\n",
      "small 130\n",
      "workin 130\n",
      "knees 130\n",
      "needs 129\n",
      "cat 129\n",
      "telling 129\n",
      "booty 129\n",
      "shady 129\n",
      "mary 129\n",
      "leaving 128\n",
      "means 128\n",
      "dress 128\n",
      "burning 128\n",
      "shout 128\n",
      "magic 128\n",
      "benz 128\n",
      "miles 127\n",
      "asked 127\n",
      "monster 127\n",
      "nowhere 126\n",
      "doo 126\n",
      "wet 126\n",
      "teeth 126\n",
      "bottom 126\n",
      "bobby 126\n",
      "watching 125\n",
      "tear 125\n",
      "warm 125\n",
      "clear 125\n",
      "box 125\n",
      "faith 125\n",
      "strange 124\n",
      "rather 124\n",
      "near 124\n",
      "yesterday 124\n",
      "knock 124\n",
      "hee 124\n",
      "boss 124\n",
      "angel 123\n",
      "known 123\n",
      "f 123\n",
      "icky 123\n",
      "hour 122\n",
      "answer 122\n",
      "clouds 122\n",
      "takin 122\n",
      "glad 122\n",
      "kid 121\n",
      "bye 121\n",
      "wife 121\n",
      "sell 121\n",
      "yeezy 121\n",
      "songs 120\n",
      "changes 120\n",
      "park 120\n",
      "mess 120\n",
      "cake 120\n",
      "seven 119\n",
      "grow 119\n",
      "jeans 119\n",
      "stuck 118\n",
      "ways 118\n",
      "middle 118\n",
      "point 118\n",
      "uhh 118\n",
      "empty 117\n",
      "single 117\n",
      "walked 117\n",
      "chain 117\n",
      "played 117\n",
      "k 117\n",
      "bird 116\n",
      "none 116\n",
      "versace 116\n",
      "sight 115\n",
      "type 115\n",
      "clean 115\n",
      "cried 115\n",
      "hop 115\n",
      "weak 115\n",
      "shots 115\n",
      "finally 115\n",
      "mi 115\n",
      "rolling 114\n",
      "sister 114\n",
      "plus 114\n",
      "news 113\n",
      "job 113\n",
      "lit 113\n",
      "john 113\n",
      "south 113\n",
      "mommy 113\n",
      "team 113\n",
      "khaled 113\n",
      "sorrow 112\n",
      "tongue 112\n",
      "fact 112\n",
      "quit 112\n",
      "fish 112\n",
      "blind 112\n",
      "handle 112\n",
      "band 111\n",
      "wine 111\n",
      "daughter 111\n",
      "secret 111\n",
      "motherfuckin 111\n",
      "mom 110\n",
      "fix 110\n",
      "movie 110\n",
      "needed 110\n",
      "unless 110\n",
      "hours 110\n",
      "hearts 110\n",
      "fingers 110\n",
      "playin 110\n",
      "christmas 109\n",
      "dry 109\n",
      "soft 109\n",
      "heat 109\n",
      "later 108\n",
      "deal 108\n",
      "return 108\n",
      "ahead 107\n",
      "changed 107\n",
      "haters 107\n",
      "rapper 107\n",
      "grown 106\n",
      "dope 106\n",
      "race 106\n",
      "hook 106\n",
      "chest 105\n",
      "less 105\n",
      "brought 105\n",
      "mountain 105\n",
      "vibe 105\n",
      "bar 105\n",
      "hoo 105\n",
      "dumb 104\n",
      "radio 104\n",
      "suck 104\n",
      "holding 104\n",
      "table 104\n",
      "price 104\n",
      "twice 104\n",
      "thee 104\n",
      "storm 104\n",
      "longer 104\n",
      "loud 103\n",
      "lives 103\n",
      "que 103\n",
      "mo 103\n",
      "birds 102\n",
      "realize 102\n",
      "scene 102\n",
      "country 102\n",
      "spirit 102\n",
      "hallelujah 102\n",
      "sent 102\n",
      "stuff 102\n",
      "ta 102\n",
      "poor 101\n",
      "v 101\n",
      "blues 101\n",
      "double 101\n",
      "chains 101\n",
      "dig 101\n",
      "feelings 100\n",
      "share 100\n",
      "crowd 100\n",
      "slim 100\n",
      "aye 100\n",
      "pretend 99\n",
      "forgive 99\n",
      "grave 99\n",
      "killed 99\n",
      "count 99\n",
      "shining 99\n",
      "tellin 99\n",
      "guns 99\n",
      "angels 99\n",
      "sunshine 99\n",
      "harder 99\n",
      "weezy 99\n",
      "loose 98\n",
      "driving 98\n",
      "class 98\n",
      "extra 98\n",
      "married 98\n",
      "happen 98\n",
      "crew 98\n",
      "expect 98\n",
      "amy 98\n",
      "waste 97\n",
      "lead 97\n",
      "shame 97\n",
      "rings 97\n",
      "hungry 97\n",
      "fit 97\n",
      "funky 97\n",
      "purple 97\n",
      "prince 97\n",
      "week 97\n",
      "heavy 96\n",
      "cops 96\n",
      "wave 96\n",
      "rush 96\n",
      "woah 96\n",
      "question 96\n",
      "sitting 96\n",
      "sir 96\n",
      "cream 96\n",
      "anyone 95\n",
      "ends 95\n",
      "state 95\n",
      "flowers 95\n",
      "silver 95\n",
      "crib 95\n",
      "switch 95\n",
      "notorious 95\n",
      "id 94\n",
      "sense 94\n",
      "figure 94\n",
      "walkin 94\n",
      "smell 94\n",
      "trouble 94\n",
      "wide 93\n",
      "wrote 93\n",
      "clock 93\n",
      "spent 93\n",
      "blessed 93\n",
      "sunday 93\n",
      "locked 93\n",
      "short 92\n",
      "bite 92\n",
      "pictures 92\n",
      "happened 92\n",
      "learned 92\n",
      "safe 92\n",
      "dollar 92\n",
      "special 92\n",
      "places 92\n",
      "poppin 92\n",
      "nose 92\n",
      "bom 92\n",
      "ive 92\n",
      "choose 91\n",
      "rule 91\n",
      "hanging 91\n",
      "isn 91\n",
      "anyway 91\n",
      "doubt 91\n",
      "fighting 91\n",
      "ur 91\n",
      "sleeping 90\n",
      "rules 90\n",
      "steady 90\n",
      "j 90\n",
      "bullshit 90\n",
      "neck 90\n",
      "problem 89\n",
      "cover 89\n",
      "lucky 89\n",
      "smokin 89\n",
      "snow 89\n",
      "church 89\n",
      "beef 89\n",
      "worst 88\n",
      "trip 88\n",
      "begin 88\n",
      "instead 88\n",
      "glass 88\n",
      "winter 88\n",
      "fo 88\n",
      "aah 88\n",
      "dem 88\n",
      "couldnt 88\n",
      "arm 87\n",
      "problems 87\n",
      "store 87\n",
      "ago 87\n",
      "pink 87\n",
      "tall 87\n",
      "pocket 87\n",
      "flip 87\n",
      "louis 87\n",
      "theres 87\n",
      "kept 86\n",
      "faces 86\n",
      "moving 86\n",
      "working 86\n",
      "bottles 86\n",
      "mercy 86\n",
      "belong 86\n",
      "thou 86\n",
      "legs 86\n",
      "trees 86\n",
      "rose 86\n",
      "coke 86\n",
      "rat 86\n",
      "pack 86\n",
      "dirt 85\n",
      "desire 85\n",
      "happiness 85\n",
      "early 85\n",
      "plane 85\n",
      "flying 85\n",
      "er 85\n",
      "marry 85\n",
      "girlfriend 85\n",
      "holdin 85\n",
      "jay 85\n",
      "limit 85\n",
      "american 85\n",
      "greatest 84\n",
      "speed 84\n",
      "rollin 84\n",
      "st 84\n",
      "except 84\n",
      "cali 84\n",
      "distance 83\n",
      "attention 83\n",
      "having 83\n",
      "lean 83\n",
      "hill 83\n",
      "drama 83\n",
      "paradise 83\n",
      "uptown 83\n",
      "shimmy 83\n",
      "radar 83\n",
      "dying 82\n",
      "pants 82\n",
      "tv 82\n",
      "plan 82\n",
      "ship 82\n",
      "self 82\n",
      "riding 82\n",
      "grass 82\n",
      "respect 82\n",
      "rockin 82\n",
      "prove 82\n",
      "thunder 82\n",
      "dah 82\n",
      "major 82\n",
      "seat 81\n",
      "letter 81\n",
      "thoughts 81\n",
      "leaves 81\n",
      "plans 81\n",
      "beauty 81\n",
      "given 81\n",
      "darlin 81\n",
      "zone 81\n",
      "sold 81\n",
      "bells 81\n",
      "jammin 81\n",
      "cock 80\n",
      "drugs 80\n",
      "closer 80\n",
      "nine 80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f0070e836381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcnt_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcnt_r\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcountword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnt_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f0070e836381>\u001b[0m in \u001b[0;36mcountword\u001b[0;34m(word, cor)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mspi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mspi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4) word frequency caculation\n",
    "def countword(word,cor):\n",
    "    splitted = cor.split(' ')\n",
    "    cnt = 0\n",
    "    for spi in splitted:\n",
    "        if word == spi:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "for word in word_list.values():\n",
    "    cnt_r = 0\n",
    "    for cor in corpus:\n",
    "        cnt_r += countword(word,cor)\n",
    "    print(word,cnt_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dietary-rogers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130250, 13)\n",
      "[  2  32  22 433 113   6 175   5 111  47   5  57 734]\n",
      "[ 32  22 433 113   6 175   5 111  47   5  57 734  19]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,tgt_input,test_size=0.2,random_state=7)\n",
    "print(enc_train.shape)\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-construction",
   "metadata": {},
   "source": [
    "# making data obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "legal-arkansas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 13), (256, 13)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-impact",
   "metadata": {},
   "source": [
    "# Making NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "composite-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "viral-judgment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 13, 12001), dtype=float32, numpy=\n",
       "array([[[-6.11649157e-05,  4.87005600e-05, -9.70188339e-05, ...,\n",
       "          1.33764072e-04, -2.98201398e-04,  1.25942257e-04],\n",
       "        [-1.99715156e-04, -4.71791282e-05, -3.07158101e-04, ...,\n",
       "          1.36504386e-04, -2.82435765e-04,  5.74917467e-05],\n",
       "        [-4.58597904e-04, -1.93435742e-04, -3.11846903e-04, ...,\n",
       "          1.41708617e-04,  9.43740304e-07,  1.24171405e-04],\n",
       "        ...,\n",
       "        [ 6.62899993e-06, -6.52272487e-04,  1.01885526e-03, ...,\n",
       "          3.61484621e-04, -2.77487503e-04,  3.37271158e-05],\n",
       "        [-1.37587005e-04, -6.38147700e-04,  1.42176403e-03, ...,\n",
       "          2.88266077e-04, -2.92127195e-04,  8.97592545e-05],\n",
       "        [-3.01854918e-04, -6.42113446e-04,  1.76324463e-03, ...,\n",
       "          1.96969864e-04, -2.57929234e-04,  1.73134118e-04]],\n",
       "\n",
       "       [[-6.11649157e-05,  4.87005600e-05, -9.70188339e-05, ...,\n",
       "          1.33764072e-04, -2.98201398e-04,  1.25942257e-04],\n",
       "        [-3.43796070e-04,  6.63922110e-05, -3.24246939e-04, ...,\n",
       "         -1.34063330e-05, -5.00717608e-04,  9.31297400e-05],\n",
       "        [-8.38777923e-05, -6.28048801e-05, -2.91510078e-04, ...,\n",
       "         -1.57790360e-04, -4.97082830e-04,  1.69191171e-05],\n",
       "        ...,\n",
       "        [ 1.72139774e-03, -7.87876954e-04,  2.47609627e-04, ...,\n",
       "          5.71058714e-04, -6.16529665e-04,  9.73731978e-04],\n",
       "        [ 1.85020384e-03, -8.12682614e-04,  1.24847327e-04, ...,\n",
       "          7.66324229e-04, -6.40855404e-04,  1.24574627e-03],\n",
       "        [ 1.84829521e-03, -5.70883683e-04,  4.40919393e-04, ...,\n",
       "          8.34042497e-04, -4.55260510e-04,  1.46978523e-03]],\n",
       "\n",
       "       [[-6.11649157e-05,  4.87005600e-05, -9.70188339e-05, ...,\n",
       "          1.33764072e-04, -2.98201398e-04,  1.25942257e-04],\n",
       "        [-5.99619743e-05, -1.68253158e-04, -2.37032218e-04, ...,\n",
       "          3.51520022e-04, -5.48390381e-04,  1.16765150e-04],\n",
       "        [-4.29783831e-05, -2.69958808e-04, -1.78705421e-04, ...,\n",
       "          3.12717340e-04, -4.80451970e-04, -6.42246450e-05],\n",
       "        ...,\n",
       "        [-1.71824242e-04, -1.23173918e-03,  8.04273004e-04, ...,\n",
       "          1.16062722e-04, -4.88188351e-04, -2.18617293e-04],\n",
       "        [-4.36498667e-04, -1.18426350e-03,  1.25070673e-03, ...,\n",
       "          9.38906087e-05, -4.72852029e-04, -9.76215088e-05],\n",
       "        [-6.95923227e-04, -1.13698200e-03,  1.63424923e-03, ...,\n",
       "          5.68163487e-05, -4.23511374e-04,  4.11901965e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-6.11649157e-05,  4.87005600e-05, -9.70188339e-05, ...,\n",
       "          1.33764072e-04, -2.98201398e-04,  1.25942257e-04],\n",
       "        [-5.99619743e-05, -1.68253158e-04, -2.37032218e-04, ...,\n",
       "          3.51520022e-04, -5.48390381e-04,  1.16765150e-04],\n",
       "        [ 2.76800507e-04, -4.53418586e-04, -2.04504584e-04, ...,\n",
       "          6.01106673e-04, -6.82096754e-04,  1.05787229e-04],\n",
       "        ...,\n",
       "        [-1.44374324e-04, -1.22329185e-03,  9.76237527e-04, ...,\n",
       "          4.96476190e-04, -8.32526304e-04, -3.40197585e-04],\n",
       "        [-3.91288748e-04, -1.23004045e-03,  1.35232322e-03, ...,\n",
       "          3.37835314e-04, -7.22311786e-04, -1.24628234e-04],\n",
       "        [-6.27684756e-04, -1.23902434e-03,  1.66857021e-03, ...,\n",
       "          1.81261988e-04, -5.92452299e-04,  9.57830853e-05]],\n",
       "\n",
       "       [[-6.11649157e-05,  4.87005600e-05, -9.70188339e-05, ...,\n",
       "          1.33764072e-04, -2.98201398e-04,  1.25942257e-04],\n",
       "        [-2.70140503e-04,  2.02214957e-04, -1.52315639e-04, ...,\n",
       "          4.51634871e-04, -5.97897219e-04,  1.28747706e-04],\n",
       "        [-5.55069128e-04, -3.49961956e-05, -2.78290769e-04, ...,\n",
       "          1.30833476e-04, -6.57918805e-04, -1.54519119e-04],\n",
       "        ...,\n",
       "        [ 2.92211713e-04, -1.03321753e-03, -1.40540025e-04, ...,\n",
       "          1.41799275e-03,  6.83314807e-04, -6.37264515e-04],\n",
       "        [ 1.17840595e-04, -9.40440106e-04, -5.61338566e-05, ...,\n",
       "          1.58696668e-03,  9.94472764e-04, -7.02607678e-04],\n",
       "        [ 1.83479151e-05, -7.29270338e-04, -1.09619701e-04, ...,\n",
       "          1.63521932e-03,  1.30199222e-03, -8.64991511e-04]],\n",
       "\n",
       "       [[-6.11649157e-05,  4.87005600e-05, -9.70188339e-05, ...,\n",
       "          1.33764072e-04, -2.98201398e-04,  1.25942257e-04],\n",
       "        [-2.32422070e-04,  2.12372091e-04, -1.30706379e-04, ...,\n",
       "          1.33241148e-04, -5.01607312e-04,  1.71433596e-04],\n",
       "        [-2.89551885e-04,  1.82483909e-05, -2.44701572e-04, ...,\n",
       "          2.71041965e-04, -6.83172606e-04,  1.39663069e-04],\n",
       "        ...,\n",
       "        [ 9.56538948e-04, -8.65235983e-04,  5.34360406e-05, ...,\n",
       "          1.06235508e-04,  5.95695572e-04,  4.14668117e-04],\n",
       "        [ 7.11189350e-04, -9.40980448e-04,  5.59009321e-04, ...,\n",
       "          1.28764150e-04,  3.72488896e-04,  4.88065649e-04],\n",
       "        [ 4.13522561e-04, -9.82787926e-04,  1.02997210e-03, ...,\n",
       "          1.37823226e-04,  1.79888448e-04,  5.78025181e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-headquarters",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "strange-object",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "508/508 [==============================] - 151s 292ms/step - loss: 4.3327\n",
      "Epoch 2/30\n",
      "508/508 [==============================] - 151s 296ms/step - loss: 3.3537\n",
      "Epoch 3/30\n",
      "508/508 [==============================] - 151s 297ms/step - loss: 3.1524\n",
      "Epoch 4/30\n",
      "508/508 [==============================] - 151s 297ms/step - loss: 3.0114\n",
      "Epoch 5/30\n",
      "508/508 [==============================] - 151s 298ms/step - loss: 2.8918\n",
      "Epoch 6/30\n",
      "508/508 [==============================] - 151s 298ms/step - loss: 2.7905\n",
      "Epoch 7/30\n",
      "508/508 [==============================] - 150s 296ms/step - loss: 2.6865\n",
      "Epoch 8/30\n",
      "508/508 [==============================] - 151s 297ms/step - loss: 2.5992\n",
      "Epoch 9/30\n",
      "508/508 [==============================] - 151s 296ms/step - loss: 2.5085\n",
      "Epoch 10/30\n",
      "508/508 [==============================] - 151s 297ms/step - loss: 2.4283\n",
      "Epoch 11/30\n",
      " 21/508 [>.............................] - ETA: 2:24 - loss: 2.3267"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-53bb5dbd7ade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "indoor-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pacific-offset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> my name is <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> my\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-sport",
   "metadata": {},
   "source": [
    "# Qustion\n",
    "\n",
    "tokenizer과정을 살펴보면 사람의 기호들을 생략하는 과정들이 담겨있다. 이를테면 '!','?','^^'등의 기호들을 생략하고 있으며 나머지 알파벳 단어들만 모두 학습에 필요한 데이터로 고려하고 있다. 하지만 생략된 기호들은 실제 사람의 언어와 대화에서 모두 의미를 갖고 있으며 말하고자 하는 의미를 전달하는데 다소 중요한 작용을 한다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
